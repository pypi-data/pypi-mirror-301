{
  "_submission_hash": "55567660c1534ca3d368b2872e302aab1b26cb83d3f764b65a77c0e05119beb8",
  "_results_hash": "52f3142a1f80fcbd17101bef14179180e28dead7c448c1867faa157c9c7abacc",
  "metadata": {
    "name": "Llama 3 70B Instruct",
    "authors": "Meta",
    "url": "https://ai.meta.com/blog/meta-llama-3/",
    "citation": "Meta, 2024",
    "type": "FOUNDATION",
    "context": 8192,
    "is_trained_for_function_calling": false,
    "details": "meta-llama/Meta-Llama-3-70B-Instruct"
  },
  "closedbook": {
    "acc": {
      "loose": 0.46615229259616364,
      "strict": 0.06767955801104972
    },
    "rouge": {
      "rouge1": {
        "precision": 0.4631332362598857,
        "recall": 0.5371821850032036,
        "fscore": 0.46289360261572743
      },
      "rouge2": {
        "precision": 0.2579209125801027,
        "recall": 0.3004963517133437,
        "fscore": 0.2636216329275711
      },
      "rougeL": {
        "precision": 0.3880671846191338,
        "recall": 0.4489699054327381,
        "fscore": 0.3866726627735914
      }
    },
    "bleurt": 0.47776114855229196,
    "gpt": 0.1574585635359116
  },
  "openbook": {
    "acc": {
      "loose": 0.4682574924914193,
      "strict": 0.0925414364640884
    },
    "rouge": {
      "rouge1": {
        "precision": 0.22408623080672696,
        "recall": 0.5483560960567966,
        "fscore": 0.2818457602025324
      },
      "rouge2": {
        "precision": 0.11670773659730314,
        "recall": 0.26196485918068046,
        "fscore": 0.14296149842203265
      },
      "rougeL": {
        "precision": 0.19429245032731035,
        "recall": 0.47541757803293877,
        "fscore": 0.24302232905117382
      }
    },
    "bleurt": 0.4730032659881846,
    "gpt": 0.22099447513812154
  },
  "evidenceprovided": {
    "acc": {
      "loose": 0.5729133597285995,
      "strict": 0.1132596685082873
    },
    "rouge": {
      "rouge1": {
        "precision": 0.4851840263294143,
        "recall": 0.6383790378446155,
        "fscore": 0.4999906343132503
      },
      "rouge2": {
        "precision": 0.26752242447520685,
        "recall": 0.3606244592230509,
        "fscore": 0.2852580524378716
      },
      "rougeL": {
        "precision": 0.3918322597257888,
        "recall": 0.517388793407006,
        "fscore": 0.40399234960750435
      }
    },
    "bleurt": 0.5209101250508378,
    "gpt": 0.27071823204419887
  }
}