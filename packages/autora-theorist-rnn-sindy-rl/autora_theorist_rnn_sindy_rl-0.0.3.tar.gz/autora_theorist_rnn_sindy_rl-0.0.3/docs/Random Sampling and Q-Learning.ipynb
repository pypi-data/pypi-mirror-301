{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Random Sampling and Q-Learning\n",
    "\n",
    "Here, we randomly sample reward sequences and simulate participants via q-learning agents in a two-armed-bandit task.\n",
    "We also use the state mechanic of the AutoRA package.\n",
    "\n",
    "### Sampler\n",
    "The sampler creates random reward sequences with an initial reward probabilities and change sampled from a gaussian distribution. The reward sequences are independent for each arm of the bandit task.\n",
    "\n",
    "## Install"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Uncomment the following line when running on Google Colab\n",
    "# !pip install autora\n",
    "# !pip install autora-experimentalist-bandit-random\n",
    "# !pip install autora-theorist-rnn-sindy-rl\n",
    "# !pip install matplotlib"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# General AutoRA\n",
    "from autora.variable import VariableCollection, Variable\n",
    "from autora.state import StandardState, on_state, Delta\n",
    "\n",
    "# Experimentalists\n",
    "from autora.experimentalist.bandit_random import bandit_random_pool\n",
    "\n",
    "# Experiment Runner\n",
    "from autora.experiment_runner.synthetic.psychology.q_learning import q_learning\n",
    "\n",
    "# Theorist\n",
    "from autora.theorist.rnn_sindy_rl import RNNSindy\n",
    "from autora.theorist.rnn_sindy_rl.utils.parse import parse\n",
    "\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Constants\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRIALS_PER_PARTICIPANTS = 100\n",
    "SAMPLES_PER_CYCLE = 1\n",
    "PARTICIPANTS_PER_CYCLE = 40\n",
    "CYCLES = 2\n",
    "INITIAL_REWARD_PROBABILITY_RANGE = [.2, .8]  # Range to sample from for the initial reward probability\n",
    "SIGMA_RANGE = [.2, .2] # Range to sample for the sigma of the gaussian distribution for the change\n",
    "\n",
    "EPOCHS = 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Variables\n",
    "\n",
    "To define our experiment, we first setup the independent and dependent variables:\n",
    "\n",
    "independent variable is \"reward-trajectory\": A 2 x n_trials Vector with entries between 0 and 1\n",
    "dependent variable is \"choice-trajectory\": A 2 x n_trials Vector with boolean entries (one hot encoded)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "variables = VariableCollection(\n",
    "    independent_variables=[Variable(name=\"reward-trajectory\")],\n",
    "    dependent_variables=[Variable(name=\"choice-trajectory\")]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## State\n",
    "We initialize a StandardState with the variablse"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "state = StandardState(variables=variables)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Components\n",
    "### Experimentalist: Random Pool\n",
    "\n",
    "We use the sampler provided in `autora-experimentalist-bandit-random` and wrap it with the on_state functionality.\n",
    "Here, we use ranomly sample one condition and use it for `PARTICIPANT_PER_CYCLE`-number of participant \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@on_state()\n",
    "def pool_on_state(num_samples, n_trials=TRIALS_PER_PARTICIPANTS):\n",
    "    \"\"\"\n",
    "    This is creates `num_samples` randomized reward trajectories of length `n_trials`\n",
    "    \"\"\"\n",
    "    # sample sigma\n",
    "    sigma = np.random.uniform(SIGMA_RANGE[0], SIGMA_RANGE[1])\n",
    "    # create a reward-trajectory\n",
    "    _trajectory_array = bandit_random_pool(\n",
    "        num_rewards=2,\n",
    "        sequence_length=n_trials,\n",
    "        initial_probabilities=[INITIAL_REWARD_PROBABILITY_RANGE, INITIAL_REWARD_PROBABILITY_RANGE],\n",
    "        sigmas=[sigma, sigma],\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    # Duplicate the same condition `PARTICIPANT_PER_CYCLE` times:\n",
    "    trajectory_array = [_trajectory_array[0] for _ in range(PARTICIPANTS_PER_CYCLE)]\n",
    "    # Convert the list into a pandas dataFrame. The column name is the same as the name of the independent variable\n",
    "    trajectory_df = pd.DataFrame({'reward-trajectory': trajectory_array})\n",
    "    return Delta(conditions=trajectory_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Runner: Q-Learning\n",
    "\n",
    "We use a q-learning runner from the `autora-synthetic-q-learning` package to simulate data on the created condition."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "runner = q_learning()\n",
    "\n",
    "@on_state()\n",
    "def runner_on_state(conditions):\n",
    "    choices, choice_probabilities = runner.run(conditions, return_choice_probabilities=True)\n",
    "    experiment_data = pd.DataFrame({\n",
    "        'reward-trajectory': conditions['reward-trajectory'].tolist(),\n",
    "        'choice-trajectory': choices,\n",
    "        'choice-probability-trajectory': choice_probabilities\n",
    "    })\n",
    "    return Delta(experiment_data=experiment_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Theorist: RNNSindy\n",
    "\n",
    "We initilize the theorist and wrap it to run on state: "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "theorist = RNNSindy(2, epochs=EPOCHS)\n",
    "\n",
    "@on_state()\n",
    "def theorist_on_state(X, y):\n",
    "    return Delta(models=[theorist.fit(X, y)])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Cycle\n",
    "After defining the components, we can run them (here, in a cycle):"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for c in range(1, CYCLES + 1):\n",
    "    print(f'Starting Cycle {c}')\n",
    "    # Creating conditions:\n",
    "    state = pool_on_state(state, num_samples=SAMPLES_PER_CYCLE)\n",
    "    # Creating experiment_data:\n",
    "    state = runner_on_state(state)\n",
    "    # Creating a model:\n",
    "    state = theorist_on_state(state)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plotting the Data and Predictions\n"
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "model = state.models[-1]\n",
    "exp_data = state.experiment_data\n",
    "    \n",
    "single_agent_choice_probability = np.array(\n",
    "        exp_data['choice-probability-trajectory'].tolist()[-1])\n",
    "single_agent_choices = np.array(exp_data['choice-trajectory'].tolist()[-1])\n",
    "single_agent_rewards = np.array(exp_data['reward-trajectory'].tolist()[-1])\n",
    "y_pred_single = np.array(\n",
    "        model.predict([single_agent_rewards], observations=[single_agent_choices])).reshape(\n",
    "        TRIALS_PER_PARTICIPANTS, 2)\n",
    "y_pred_list = [y[0] for y in y_pred_single]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# plot:\n",
    "ax.plot(range(TRIALS_PER_PARTICIPANTS), single_agent_choice_probability[:,0], label='Choice Probability')\n",
    "\n",
    "ax.plot(range(TRIALS_PER_PARTICIPANTS), y_pred_list, label='Prediction')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Trial')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Single Agent Choice Probability and Model Prediction')\n",
    "\n",
    "# Legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Discovered Learning Equation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "equations = (parse(model))\n",
    "\n",
    "eq_chosen = equations[0]\n",
    "eq_non_chosen = equations[1]\n",
    "\n",
    "\n",
    "print(eq_chosen)\n",
    "print(eq_non_chosen)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
