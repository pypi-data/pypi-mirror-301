{
    "degree_centrality": {
        "technical": "The function computes the degree centrality for nodes in a given graph. Degree centrality for a node is the proportion of nodes it is directly connected to. The output is a dictionary where nodes are keys and their degree centrality values are the corresponding values. The degree centrality values are normalized by dividing by the maximum possible degree in a simple graph (n-1 where n is the number of nodes). In cases of multigraphs or graphs with self-loops, the maximum degree can exceed n-1, allowing for degree centrality values greater than 1.",
        "colloquial": "To put it simply, this function calculates how important each node is in a network by looking at how many connections it has compared to the total possible connections it could have. The result is a dictionary showing the degree centrality value for each node in the graph."
    },
    "closeness_centrality": {
        "technical": "The closeness centrality of a node is calculated as the reciprocal of the average shortest path distance to that node over all reachable nodes. The formula for closeness centrality is defined as the number of reachable nodes minus one divided by the sum of the shortest path distances from each reachable node to the target node. Higher values of closeness indicate higher centrality. For graphs with multiple connected components, Wasserman and Faust proposed an improved formula that considers the ratio of reachable actors to the average distance from those actors. This scale factor ensures that nodes from small components receive a smaller closeness value. The function takes a NetworkX graph `G` and an optional node `u` as parameters, and can use a specified edge attribute key for distance calculations. The `wf_improved` parameter allows for scaling by the fraction of reachable nodes, following the Wasserman and Faust improved formula. The function returns a dictionary of nodes with their closeness centrality values.",
        "colloquial": "Compute how easily each node can reach all other nodes in the network. The closeness centrality of a node is determined by the average shortest path distance to that node from all other reachable nodes. Higher values indicate higher centrality. For graphs with multiple connected components, a modified formula by Wasserman and Faust is suggested. This formula considers the ratio of reachable actors to the average distance from those actors. The function takes into account the edge attribute for distance calculation and can be adjusted to use the improved formula by setting a parameter. The result is a dictionary of nodes with their closeness centrality values."
    },
    "pagerank": {
        "technical": "The function calculates the PageRank of nodes in a graph by ranking them based on the structure of incoming links. It uses a damping parameter for PageRank, a personalization vector for specific nodes, and allows for setting various parameters like maximum iterations, error tolerance, starting values, edge weights, and handling dangling nodes. The function returns a dictionary with nodes as keys and their corresponding PageRank values.",
        "colloquial": "This function calculates the PageRank of nodes in a graph, which is a way to rank them based on their connections. It uses a damping parameter and can consider personalization values for specific nodes. The algorithm may not always converge, and there are options to set maximum iterations and error tolerance."
    },
    "eigenvector_centrality": {
        "technical": "The function computes the eigenvector centrality for a given graph by iteratively calculating the centrality of each node based on the centrality of its neighbors. It utilizes a left eigenvector associated with the maximum modulus eigenvalue that is positive. The eigenvector centrality for a node is determined by summing the centrality values of its predecessors. The algorithm uses power iteration with specified parameters like maximum iterations and error tolerance to converge to the dominant eigenvector. The resulting dictionary contains nodes with their corresponding eigenvector centrality values, ensuring that the vector has a unit Euclidean norm and non-negative values.",
        "colloquial": "To put it simply, eigenvector centrality calculates how important a node is in a network by considering the importance of its connections. It looks at how well-connected a node is to other important nodes in the network."
    },
    "load_centrality": {
        "technical": "Compute the load centrality for nodes in a graph. The load centrality of a node is the fraction of all shortest paths that pass through that node. The function takes parameters such as the graph G, a boolean for normalization, an optional weight attribute for considering edge weights, and a cutoff parameter for limiting path lengths. It returns a dictionary with nodes as keys and their corresponding centrality values. Load centrality is distinct from betweenness centrality and was initially proposed by a specific source.",
        "colloquial": "Compute the importance of nodes by looking at how many shortest paths go through them. This helps identify nodes that act as critical connectors in a network. The function calculates this centrality measure for each node in a graph."
    },
    "harmonic_centrality": {
        "technical": "Compute harmonic centrality for nodes by summing the reciprocal of the shortest path distances from all other nodes to a specific node `u`. The formula for harmonic centrality is $C(u) = \\sigma(1/d(v, u))$, where $d(v, u)$ represents the shortest-path distance between nodes v and u. If specified, harmonic centrality values can be calculated based on the reciprocals of the shortest path distances from a subset of nodes (sources) to node u. Higher values of harmonic centrality indicate higher centrality of a node. The function takes a NetworkX graph G, a container of nodes nbunch for which centrality values are calculated, a container of source nodes for reciprocal distance computation, and an optional edge attribute key for distance calculation. The function returns a dictionary with nodes as keys and their harmonic centrality values as the corresponding values. If the 'distance' keyword is provided, Dijkstra's algorithm is used with the specified edge attribute as the edge weight for shortest path length computation.",
        "colloquial": "Calculate harmonic centrality for nodes in a graph. Harmonic centrality of a node 'u' is the sum of the reciprocal of the shortest path distances from all other nodes to 'u'. Higher values mean higher centrality. You can specify certain nodes as sources for the calculations. The function returns a dictionary with nodes as keys and their harmonic centrality values as the corresponding values."
    },
    "shortest_path": {
        "technical": "The function computes shortest paths in a graph. It allows for specifying a starting node, an ending node, edge weights, and the algorithm to use for path computation. The output can be a list or dictionary of shortest paths, including both the source and target nodes. It handles cases where only the source, only the target, or neither are specified. It raises exceptions for cases like a missing source node or an unsupported algorithm. The function is part of NetworkX library and is useful for various graph-related computations.",
        "colloquial": "This function calculates the shortest paths in a graph. You can specify the starting and ending nodes, choose the weight for edges, and the algorithm to use. It returns the shortest path from the specified source to target nodes or all possible combinations. If no source or target is given, it provides paths for all nodes."
    },
    "voterank": {
        "technical": "The function `voterank` uses the VoteRank algorithm to rank nodes in a graph based on a voting scheme. It iteratively elects nodes with the highest votes from their in-neighbors, decreasing the voting ability of out-neighbors of elected nodes in subsequent turns. The function takes a NetworkX graph `G` and an optional integer parameter `number_of_nodes` to extract a specified number of top-ranked nodes. The returned list `voterank` contains the ordered list of computed influential nodes with positive votes. The algorithm works for both undirected and directed graphs, with the directed version having nodes vote only for their in-neighbors and updating the voting ability of the elected node and its out-neighbors.",
        "colloquial": "The VoteRank algorithm helps find important nodes in a network by having nodes vote for their neighbors. The node with the most votes is chosen first, then the process repeats with decreasing influence from already chosen nodes. The result is a list of influential nodes in the network."
    },
    "katz_centrality": {
        "technical": "The Katz centrality algorithm computes the centrality of nodes in a graph by considering the centrality of their neighbors. It is a generalization of eigenvector centrality and is calculated using the formula $x_i = \\alpha \\sum_{j} A_{ij} x_j + \\beta$, where $A$ is the adjacency matrix of the graph and $\\alpha$ is an attenuation factor. The parameter $\\beta$ controls the initial centrality, and $\\alpha$ must be less than the inverse largest eigenvalue of the adjacency matrix for accurate computation. Katz centrality measures the relative influence of a node within a network by considering both immediate neighbors and all other nodes connected to the node through these neighbors. Weight can be assigned to immediate neighbors using $\\beta$, while connections through distant neighbors are penalized by $\\alpha$.",
        "colloquial": "To put it simply, Katz centrality calculates how important a node is in a network by looking at how many paths connect it to other nodes. It considers both direct connections and connections through other nodes. The formula involves an attenuation factor and a weight for immediate neighbors. The centrality value is influenced by the centrality of neighboring nodes."
    },
    "k_core": {
        "technical": "The function returns the k-core of a given NetworkX graph, which is a maximal subgraph containing nodes with a degree of at least `k`. It is noted that the function will not accept `MultiGraph` objects in version 3.5. The main core is the core with the largest core number equal to `k`. For directed graphs, the node degree is defined as the sum of in-degree and out-degree. Graph, node, and edge attributes are preserved in the subgraph.",
        "colloquial": "This function finds the k-core of a graph, which is a subgraph where all nodes have a degree of at least k. It returns this subgraph based on the specified value of k."
    },
    "all_pairs_all_shortest_paths": {
        "technical": "Compute all shortest paths between all nodes in a graph. The function allows for specifying edge weights, using different algorithms for path length computation, and returns a generator of dictionaries containing arrays of all shortest paths between each pair of nodes.",
        "colloquial": "Calculate the shortest routes between every pair of locations in a network. You can specify the weight of each edge, choose the algorithm to use, and get a dictionary showing all the shortest paths."
    },
    "all_pairs_shortest_path_length": {
        "technical": "This function calculates the shortest path lengths between all nodes in a given NetworkX graph. It allows for specifying a depth cutoff for the search, limiting the returned paths to a certain length. The function returns an iterator with dictionaries containing the shortest path lengths from a source node to target nodes. The iterator only includes reachable node pairs.",
        "colloquial": "This function calculates the shortest distances between all pairs of nodes in a graph. You can specify a maximum depth for the search. The output is an iterator that provides the shortest path lengths between reachable node pairs."
    },
    "all_pairs_dijkstra_path_length": {
        "technical": "This function computes the shortest path lengths between all nodes in a weighted graph using Dijkstra's algorithm. It takes parameters such as the graph, a cutoff value to stop the search, and a weight attribute to determine edge weights. The function returns an iterator with dictionaries keyed by target nodes and their corresponding shortest path lengths. The example provided demonstrates how to use this function with a NetworkX graph. It is important to note that edge weight attributes must be numerical, and distances are calculated as sums of weighted edges traversed. The dictionary returned only includes keys for reachable node pairs.",
        "colloquial": "This function calculates the shortest path lengths between all pairs of nodes in a graph. You can set a cutoff to stop the search if the path length exceeds a certain value. You can specify how edge weights are accessed, either through a string key or a custom function. The output is an iterator with dictionaries containing the target node and the shortest path length from the source node to the target node."
    },
    "all_pairs_bellman_ford_path_length": {
        "technical": "Compute shortest path lengths between all nodes in a weighted graph. The function takes a NetworkX graph and a weight parameter, which can be a string or a function. If a string is provided, edge weights are accessed using the specified key. If a function is provided, it calculates the weight of an edge based on the function's output. The function returns an iterator with dictionaries containing target nodes as keys and shortest path lengths as values. The distances are calculated as the sum of weighted edges traversed, and the returned dictionary only includes reachable node pairs.",
        "colloquial": "This function calculates the shortest distances between all pairs of nodes in a graph. You can specify how edge weights are determined, either by a specific attribute or a custom function. The output is a dictionary showing the shortest path length from each source node to every other node in the graph."
    },
    "all_pairs_shortest_path": {
        "technical": "The function computes the shortest paths between all nodes in a given NetworkX graph up to a specified depth cutoff. It returns a dictionary containing the shortest paths between each pair of source and target nodes. The function may provide only one of the multiple shortest paths of the same length between two nodes. It is related to algorithms like Floyd-Warshall and all_pairs_all_shortest_paths in the NetworkX library.",
        "colloquial": "This function helps you find the shortest paths between all pairs of nodes in a graph. You can set a limit on how far the search should go. It gives you a dictionary showing the shortest paths from each source node to every other node. Just provide the graph and the desired cutoff depth, and you'll get the shortest paths as output."
    },
    "all_pairs_dijkstra_path": {
        "technical": "This function computes the shortest paths between all nodes in a weighted graph. It allows for specifying a cutoff length for the search and supports different methods for accessing edge weights. The output is an iterator containing dictionaries with target nodes as keys and the shortest paths as values. The function requires numerical edge weight attributes and calculates distances as sums of weighted edges traversed.",
        "colloquial": "Compute the shortest paths between all nodes in a graph, considering the weights of the edges. You can set a cutoff length for the paths, and specify how the edge weights are calculated. The function returns an iterator with the shortest paths from each source node to all other nodes in the graph."
    },
    "all_pairs_bellman_ford_path": {
        "technical": "Compute shortest paths between all nodes in a weighted graph using the Bellman-Ford algorithm. The function takes a NetworkX graph and a weight parameter, which can be a string or a function. If a string, edge weights are accessed through the specified key; if a function, the weight is determined by the function. The output is an iterator containing dictionaries with targets as keys and shortest paths as values. The algorithm assumes numerical edge weight attributes and calculates distances as sums of weighted edges traversed.",
        "colloquial": "Compute the shortest paths between all nodes in a weighted graph. You can specify how to access edge weights in the graph. The function returns an iterator with dictionaries that have the target node as the key and the shortest path as the value. Make sure edge weight attributes are numerical."
    },
    "find_cliques_recursive": {
        "technical": "The function returns an iterator over all maximal cliques in a graph. A maximal clique for a node is the largest complete subgraph containing that node. The largest maximal clique is known as the maximum clique. It is a recursive implementation that may face recursion depth issues. It accepts a list of nodes and only returns maximal cliques containing all of these nodes. This can speed up the process if specific cliques are of interest. The algorithm avoids storing all cliques in memory by keeping only current candidate node lists during the search. The implementation is based on the Bron-Kerbosch algorithm with adaptations by Tomita, Tanaka, and Takahashi.",
        "colloquial": "This function finds all the biggest groups of connected nodes in a graph. It returns these groups one by one, and each group is a list of nodes that are fully connected to each other. It works by checking each node and its connections to find the largest possible group. If you only want groups that include specific nodes, you can provide a list of those nodes. This function is recursive, meaning it calls itself to find the groups, which can be slow for very large graphs. If you prefer a faster non-recursive version, check out the `find_cliques` function."
    },
    "average_clustering": {
        "technical": "The function computes the average clustering coefficient for a given graph G. The clustering coefficient is the average of local clustering coefficients for all nodes in the graph. It is calculated as the sum of local clustering coefficients divided by the number of nodes. The function allows for specifying a subset of nodes to compute the average clustering for and also considers edge weights if provided. Additionally, there is an option to exclude nodes with zero clustering values from the average calculation. The function returns the average clustering coefficient as a float value. It is noted that self-loops are not considered in the computation.",
        "colloquial": "To put it simply, this function calculates how tightly connected nodes are in a graph. It finds the average clustering coefficient for the graph, which is a measure of how much nodes tend to form clusters. The result is the average of the local clustering coefficients of all nodes in the graph."
    },
    "transitivity": {
        "technical": "Compute the graph transitivity by calculating the fraction of all possible triangles present in the graph. This is done by identifying the number of \"triads\" (two edges with a shared vertex) and then using the formula: T = 3*(number of triangles)/(number of triads). The function takes a graph G as a parameter and returns the transitivity as a float value. Ignore self loops in the calculations.",
        "colloquial": "Calculate how interconnected the graph is by looking at the number of triangles compared to the number of possible triangles. This gives you the graph's transitivity. Just remember to ignore any loops in the graph."
    },
    "effective_graph_resistance": {
        "technical": "The function calculates the Effective graph resistance, also known as the Kirchhoff index, of a given NetworkX graph. It sums the resistance distance between every pair of nodes in the graph. If no weights are provided, a weight of 1 is assigned to all edges. The effective graph resistance is infinite for disconnected graphs. The function allows for specifying edge weights and inverting weights for proper resistance distance calculation. The result is the effective graph resistance value as a float. The function raises exceptions for directed graphs and graphs without nodes. The implementation is based on Theorem 2.2, ignoring self-loops and contracting multi-edges into one with adjusted weights.",
        "colloquial": "This function calculates the Effective graph resistance of a given graph, which is also known as the Kirchhoff index. It sums up the resistance distance between every pair of nodes in the graph. If no weights are provided, it assumes each edge has a weight of 1. For disconnected graphs, the effective graph resistance is considered infinite. The function allows for specifying edge weights and inverting weights for proper resistance distance calculation. The result is the effective graph resistance value."
    },
    "degree_assortativity_coefficient": {
        "technical": "Compute the degree assortativity of a graph by measuring the similarity of connections based on node degree. The function takes a NetworkX graph as input, along with optional parameters for directed graphs (source and target node degree types), edge weight attribute, and a list of nodes to compute assortativity for. It returns a float value representing the assortativity of the graph by degree. The calculation is based on Eq. (21) in a reference, considering the joint probability distribution (mixing matrix) of node degrees.",
        "colloquial": "Calculate how similar nodes in the graph are in terms of their connections based on their degree. The function computes the degree assortativity of the graph, which is a measure of how likely nodes are to connect to other nodes with a similar degree. You can specify parameters like the type of degree for the source and target nodes, edge weight, and which nodes to include in the calculation. The function returns a float value representing the assortativity of the graph by degree."
    },
    "average_degree_connectivity": {
        "technical": "Compute the average degree connectivity of a graph by calculating the average nearest neighbor degree of nodes with a specific degree. For weighted graphs, a similar measure can be obtained using the weighted average neighbor's degree, which considers the weights of the edges connecting nodes and the weighted degrees of the nodes. The function takes parameters such as the graph, source and target specifications for directed graphs, specific nodes to compute neighbor connectivity for, and an optional weight attribute for edges. The output is a dictionary where the keys represent the degree k and the values are the corresponding average connectivity values.",
        "colloquial": "Compute the average degree connectivity of a graph by finding the average degree of the neighbors of each node. This metric helps understand how connected nodes are to their neighbors in the graph. You can specify whether to consider incoming, outgoing, or both types of connections. The function returns a dictionary where each degree value corresponds to the average connectivity of nodes with that degree."
    },
    "average_neighbor_degree": {
        "technical": "The function calculates the average degree of the neighbors of each node in a graph. For undirected graphs, the neighborhood of a node consists of nodes connected to it by an edge. In directed graphs, the neighborhood can be based on incoming, outgoing, or both types of connections. The average neighborhood degree of a node is computed by summing the degrees of its neighbors and dividing by the number of neighbors. For weighted graphs, a similar measure is calculated using edge weights. The function takes parameters such as the graph, source type, target type, nodes to consider, and edge weight attribute. It returns a dictionary where each node is mapped to the average degree of its neighbors.",
        "colloquial": "This function calculates the average degree of the neighbors of each node in a graph. For undirected graphs, it considers nodes connected to a specific node. In directed graphs, it looks at predecessors, successors, or both based on the specified parameters. The average neighbor degree is computed by summing the degrees of neighboring nodes and dividing by the total number of neighbors. Weighted graphs can also be analyzed by considering edge weights. The function returns a dictionary with each node's average neighbor degree."
    },
    "girvan_newman": {
        "technical": "The method utilizes the Girvan\u2013Newman algorithm to detect communities in a graph by iteratively removing edges based on a specified function. The function returns an iterator over tuples of sets of nodes, where each set represents a community at a particular level of the algorithm. The algorithm breaks down the graph by removing the most valuable edge, typically the one with the highest betweenness centrality, revealing the underlying community structure.",
        "colloquial": "This method helps find groups within a network by gradually removing edges. It identifies communities by taking out the most important connections one by one, revealing the underlying structure of the network."
    },
    "greedy_color": {
        "technical": "The function colors a graph using various strategies of greedy graph coloring, aiming to minimize the number of colors used while ensuring that no neighboring nodes have the same color. The chosen coloring strategy dictates the order in which nodes are colored. The function allows for different built-in strategies such as largest_first, random_sequential, smallest_last, independent_set, connected_sequential_bfs, connected_sequential_dfs, and saturation_largest_first. Additionally, it provides the option to use a color interchange algorithm.",
        "colloquial": "Color a graph by assigning colors to vertices in a way that minimizes the number of colors used, ensuring that neighboring vertices have different colors. The order in which vertices are colored is determined by a specified strategy, such as largest-first or smallest-last. The function returns a dictionary where nodes are keys and assigned colors are values."
    },
    "best_partition": {
        "technical": "The function computes the partition of the graph nodes that maximizes modularity using the Louvain heuristic. It starts with an optional initial partition of nodes and can consider a specified weight for the graph edges. The resolution parameter can adjust the community sizes, and there is an option to randomize the node and community evaluation order for different partitions. The function returns a dictionary representing the partition, with communities numbered from 0 to the number of communities.",
        "colloquial": "This function helps find the best way to divide a graph into different groups, called communities. It aims to maximize the quality of these divisions using a specific method. You can provide some initial groupings to start with, and adjust parameters to influence the size of the communities. The function can also introduce randomness to explore different possible divisions."
    },
    "resource_allocation_index": {
        "technical": "Compute the resource allocation index for each pair of nodes in the given iterable ebunch. The resource allocation index of nodes u and v is calculated as the sum of 1 over the number of neighbors shared by u and v. The function takes a NetworkX undirected graph G and an iterable of node pairs ebunch as input. If ebunch is not provided, all nonexistent edges in the graph will be considered. The function returns an iterator of 3-tuples containing the node pair (u, v) and their corresponding resource allocation index. It raises NetworkXNotImplemented if G is a directed graph or a multigraph, and NodeNotFound if any node in ebunch is not present in G.",
        "colloquial": "To put it simply, this function calculates the resource allocation index for pairs of nodes in a graph. The index is based on the shared neighbors between two nodes, with more shared neighbors indicating a higher likelihood of a connection. The function takes in a graph and a list of node pairs, and returns the resource allocation index for each pair."
    },
    "betweenness_centrality": {
        "technical": "The function computes the shortest-path betweenness centrality for nodes in a graph. Betweenness centrality of a node is calculated as the sum of the fraction of all-pairs shortest paths that pass through that node. The function allows for optional parameters such as using a subset of nodes for estimation, normalization of betweenness values, considering edge weights, including endpoints in path counts, and setting a random number generation state. The result is a dictionary where nodes are keys and their corresponding betweenness centrality values are the dictionary values.",
        "colloquial": "To put it simply, this function calculates how important a node is in connecting other nodes by looking at how many shortest paths go through it."
    },
    "diameter": {
        "technical": "The function calculates and returns the diameter of a given graph G, which represents the maximum eccentricity in the graph. The parameters include the graph G, an optional precomputed dictionary of eccentricities, and the weight parameter that can be a string, function, or None to handle edge weights. The function returns an integer value representing the diameter of the graph.",
        "colloquial": "This function calculates the diameter of a graph, which is the longest shortest path between any two nodes. It takes into account the eccentricity of the nodes in the graph. The diameter is essentially the maximum distance between any two vertices in the graph."
    },
    "radius": {
        "technical": "The function returns the radius of a given graph, which is defined as the minimum eccentricity among all nodes in the graph. The eccentricity of a node is the maximum distance between that node and any other node in the graph. The function takes a NetworkX graph as input, along with an optional precomputed dictionary of eccentricities and a weight parameter to consider edge weights in the calculations. The radius is an integer value representing the minimum eccentricity in the graph.",
        "colloquial": "The function calculates and returns the smallest distance from any node to the farthest node in a graph."
    },
    "eccentricity": {
        "technical": "The function calculates and returns the eccentricity of nodes in a given NetworkX graph. The eccentricity of a node 'v' is defined as the maximum distance from 'v' to all other nodes in the graph. The function takes parameters such as the graph 'G', the specific node 'v', shortest path lengths 'sp', and weight information. It returns a dictionary where each node is associated with its eccentricity value.",
        "colloquial": "This function calculates the eccentricity of nodes in a graph. The eccentricity of a node is the maximum distance from that node to all other nodes in the graph. You can specify a node or a set of nodes to get their eccentricity values. The function also allows for specifying weights for edges in the graph. The output is a dictionary where each node is paired with its eccentricity value."
    },
    "articulation_points": {
        "technical": "The function yields the articulation points, or cut vertices, of a graph. An articulation point is a node whose removal increases the number of connected components in the graph. A graph without articulation points is biconnected. Articulation points belong to more than one biconnected component. The algorithm uses a non-recursive depth-first-search to find articulation points and biconnected components by tracking the highest level that back edges reach in the DFS tree. An articulation point is identified if there exists a subtree rooted at the node where no back edge links to a predecessor in the DFS tree.",
        "colloquial": "The function identifies and returns the nodes in a graph that, when removed, increase the number of separate sections in the graph. These nodes are known as articulation points or cut vertices. Removing them along with their connecting edges would split the graph into more distinct parts. In a graph without these points, it is considered biconnected. Articulation points are part of multiple biconnected sections in a graph. The function uses a method that involves tracking the depth of back edges in a non-recursive depth-first search to find these points and biconnected components."
    },
    "bridges": {
        "technical": "Generate all bridges in a graph by identifying edges that, when removed, increase the number of connected components. Bridges are edges that do not belong to any cycle and are also referred to as cut-edges, isthmuses, or cut arcs. The function takes an undirected graph `G` and an optional node `root`, returning edges that disconnect the graph or increase the number of connected components. The algorithm used relies on chain decomposition to find bridges, ensuring they are not multi-edges in a simple graph. The worst-case time complexity is $O(m + n)$, where $n$ is the number of nodes and $m$ is the number of edges in the graph.",
        "colloquial": "In simple terms, this function helps identify edges in a graph that, when removed, split the graph into multiple separate parts. These edges are called bridges. The function can be used to find these critical edges in a graph."
    },
    "average_shortest_path_length": {
        "technical": "The function calculates the average shortest path length in a graph. It sums the shortest path distances between all pairs of nodes, excluding pairs of the same node, and divides by the total number of unique node pairs in the graph. The method used to compute path lengths can be specified, with options including 'unweighted', 'dijkstra', 'bellman-ford', 'floyd-warshall', and 'floyd-warshall-numpy'. An exception is raised for null graphs, disconnected graphs, or unsupported method options.",
        "colloquial": "The function calculates the average shortest path length in a network. It sums up the shortest distances between all pairs of nodes and divides it by the total number of node pairs in the network."
    },
    "barycenter": {
        "technical": "Calculate the barycenter of a connected graph, potentially considering edge weights. The barycenter, also known as the median, of a connected graph G is the subgraph formed by the nodes v that minimize the sum of distances to all other nodes u in G, where the distance is determined by the shortest path length. The barycenter can be identified using the function barycenter in NetworkX.",
        "colloquial": "Calculate the barycenter of a connected graph, which is like finding the middle point that minimizes the total distance to all other nodes. This barycenter is also known as the median of the graph."
    },
    "approximate_current_flow_betweenness_centrality": {
        "technical": "The function computes the approximate current-flow betweenness centrality for nodes in a graph. It approximates the current-flow betweenness centrality within an absolute error of epsilon with high probability. The function allows for normalization of betweenness values, consideration of edge weights, specification of data type, selection of linear solver type, setting of error tolerance, defining the maximum number of sample node pairs for approximation, and controlling random number generation. The function returns a dictionary of nodes with their corresponding betweenness centrality values. The algorithm's time complexity is O((1/epsilon^2)m*sqrt(k)*log(n)), and it requires O(m) space for n nodes and m edges. If edges have a 'weight' attribute, those weights are utilized in the computation.",
        "colloquial": "This function calculates an approximate measure of how important each node is in a network based on the flow of current between nodes. It aims to do this with a certain level of accuracy, using various parameters like the network structure, edge weights, and solver type. The result is a dictionary where each node is assigned a value representing its centrality in the network."
    },
    "current_flow_closeness_centrality": {
        "technical": "Compute current-flow closeness centrality for nodes in a network by evaluating the effective resistance between them. This metric, also known as information centrality, considers the flow of electrical current between nodes to determine their centrality. The function takes into account edge weights, with options to specify the weight attribute and data type. It offers different linear solver options for computing the flow matrix, such as \"full,\" \"lu,\" or \"cg.\" The result is a dictionary mapping nodes to their current flow closeness centrality values, calculated based on the network's structure and connectivity.",
        "colloquial": "Calculate the current-flow closeness centrality for nodes in a network. This metric is based on the effective resistance between nodes and is also referred to as information centrality. The function takes a graph as input along with optional parameters like edge weights, data type, and solver type. It returns a dictionary with nodes and their corresponding current-flow closeness centrality values."
    },
    "all_pairs_lowest_common_ancestor": {
        "technical": "This function returns the lowest common ancestor of all pairs of nodes in a directed graph. It takes in a graph and an optional iterable of pairs of nodes, and yields a 2-tuple for each pair containing the pair of nodes and their lowest common ancestor. The function raises an error if the graph is null or not a directed acyclic graph.",
        "colloquial": "This function finds the lowest common ancestor for all pairs of nodes in a tree. It returns the least common ancestor of each pair of nodes provided, or for all pairs if none are specified. The order of the node pairs does not matter."
    },
    "tree_all_pairs_lowest_common_ancestor": {
        "technical": "The function yields the lowest common ancestor for sets of pairs in a tree. It takes a directed graph representing a tree, an optional root node, and an iterable of pairs of nodes as input. The output is a generator of tuples where each tuple consists of a pair of nodes and their lowest common ancestor. The function uses Tarjan's off-line lowest-common-ancestors algorithm and has a time complexity of O(4 * (V + E + P)), where P is the number of pairs requested.",
        "colloquial": "This function finds the lowest common ancestor for different pairs of nodes in a tree. It takes in a directed graph representing the tree, an optional root node, and an optional set of pairs of nodes. The function returns a generator that provides tuples of pairs of nodes along with their lowest common ancestor. The algorithm used is Tarjan's off-line lowest-common-ancestors algorithm, which runs efficiently."
    },
    "capacity_scaling": {
        "technical": "The algorithm finds a minimum cost flow that satisfies all demands in a directed graph. It utilizes a capacity scaling successive shortest augmenting path approach. The graph contains nodes with demands, where negative demands indicate flow out of the node and positive demands indicate flow into the node. The algorithm requires attributes for nodes (demand), edges (capacity, weight), and allows for customization of the heap type used. It returns the cost of the minimum cost flow and a dictionary representing the flow on each edge. Exceptions are raised for unfeasible or unbounded scenarios. The algorithm is not suitable for floating-point edge weights.",
        "colloquial": "To find the cheapest way to send the required flow from one point to another in a network, you can use the capacity scaling successive shortest augmenting path algorithm. This algorithm considers the costs and capacities of edges in the network, as well as the demands at each node. By ensuring that the total demand across all nodes is balanced (sums up to zero), it calculates the minimum cost flow that satisfies all demands. The algorithm returns the cost of this minimum flow and provides a dictionary showing the flow on each edge in the network."
    },
    "rich_club_coefficient": {
        "technical": "The function calculates the rich-club coefficient for a given graph `G`. This coefficient is the ratio of the number of actual edges between nodes with a degree higher than a specified degree *k* to the number of potential edges that could exist among those nodes. The calculation is based on the formula: \\phi(k) = 2 E_k / (N_k (N_k - 1)), where `N_k` is the count of nodes with a degree greater than *k*, and `E_k` is the number of edges connecting those nodes. The function allows for normalization using a randomized network and provides the rich-club coefficient values in a dictionary keyed by degree.",
        "colloquial": "This function calculates a metric called the rich-club coefficient for a given graph. The rich-club coefficient measures how well-connected nodes with high degrees are compared to what would be expected by chance. It looks at the ratio of actual connections between high-degree nodes to the maximum possible connections. The function can also normalize the coefficient using a randomized network for comparison."
    },
    "center": {
        "technical": "The function returns the set of nodes in a graph that have the minimum eccentricity, which is equal to the radius of the graph. It takes as input a NetworkX graph, an optional precomputed dictionary of eccentricities, and a weight parameter that can be a string, function, or None to handle edge weights. The output is a list of nodes that form the center of the graph.",
        "colloquial": "This function finds the central nodes in a graph, which are the nodes that are at the same distance from all other nodes in the graph."
    },
    "jaccard_coefficient": {
        "technical": "The function computes the Jaccard coefficient for all pairs of nodes in a given graph or a specified set of node pairs. The Jaccard coefficient is calculated as the size of the intersection of neighbors divided by the size of the union of neighbors for each pair of nodes. The function returns an iterator of 3-tuples containing the node pairs and their corresponding Jaccard coefficients. It raises exceptions if the input graph is not undirected or if any node in the specified pairs is not found in the graph.",
        "colloquial": "You can use this function to calculate the Jaccard coefficient for pairs of nodes in a graph. It measures how similar two nodes are based on the number of common neighbors they share, compared to all their neighbors. Just provide the graph and the pairs of nodes you want to compare, and it will give you the Jaccard coefficient for each pair."
    },
    "adamic_adar_index": {
        "technical": "Compute the Adamic-Adar index for all pairs of nodes in a given graph. The index is calculated as the sum of the reciprocal of the logarithm of the degree of common neighbors between two nodes. This measure is designed to avoid division by zero for nodes connected only through self-loops and is suitable for networks without self-loops. The function takes a NetworkX undirected graph and an optional iterable of node pairs as input. It returns an iterator containing 3-tuples of node pairs and their corresponding Adamic-Adar index values.",
        "colloquial": "Compute a special score for pairs of nodes in a network. The score is based on how many neighbors they have in common, with more weight given to neighbors that are not well-connected. This score helps predict if two nodes are likely to be linked."
    },
    "preferential_attachment": {
        "technical": "The function computes the preferential attachment score for all pairs of nodes in the provided iterable. The preferential attachment score of nodes `u` and `v` is calculated as the product of the degrees of their respective neighbors. The function takes a NetworkX undirected graph `G` and an optional iterable of node pairs `ebunch`. If `ebunch` is not specified, the function computes the score for all non-existent edges in the graph. The function returns an iterator of 3-tuples containing the node pair `(u, v)` and their preferential attachment score `p`. It raises exceptions if the graph is a directed graph or if a node in the iterable is not found in the graph.",
        "colloquial": "Compute a score for how likely two nodes are to be connected based on how many neighbors each node has. This score is calculated for pairs of nodes in a graph."
    },
    "all_node_cuts": {
        "technical": "The function returns all minimum k cutsets of an undirected graph G by utilizing Kanevsky's algorithm. It finds the set (or sets) of nodes with cardinality equal to the node connectivity of G, which when removed, would split G into two or more connected components. The parameters include the graph G, an integer k representing the node connectivity (which can be computed if None), and a flow function for flow computations. The function provides a generator of node cutsets, each with cardinality equal to the node connectivity of the input graph. The algorithm is based on computing minimum cuts using local maximum flow computations among nodes of highest degree and non-adjacent nodes in the graph, ensuring that each minimum cut is unique.",
        "colloquial": "This function finds all the smallest groups of nodes in a graph that, if removed, would split the graph into two or more separate parts. It calculates these groups based on the connectivity of the nodes."
    },
    "wiener_index": {
        "technical": "The function calculates the Wiener index of a given graph by summing the shortest-path distances between all pairs of reachable nodes. It considers edge weights for computing these distances. If the graph is not connected, the function raises an error. For pairs of nodes that are not reachable, the distance is assumed to be infinity. The Wiener index is extended to directed graphs in this function.",
        "colloquial": "The function calculates the Wiener index of a graph, which is the total sum of the shortest distances between all pairs of nodes in the graph. If the graph is not connected, the function returns infinity."
    },
    "number_of_spanning_trees": {
        "technical": "The function calculates the number of spanning trees in a graph. For undirected graphs, it returns the total number of spanning trees in the graph. For directed graphs, it computes the number of arborescences rooted at a specified node. The function can also handle weighted graphs, where it calculates the sum of all spanning tree or arborescence weights, with the weight of a tree or arborescence being the product of its edge weights. The function utilizes Kirchoff's Tree Matrix Theorem for undirected graphs and a similar theorem (Tutte's Theorem) for directed graphs to determine the number of spanning trees or arborescences.",
        "colloquial": "This function calculates the total number of ways to create a tree that connects all the nodes in a graph. For undirected graphs, it counts the number of spanning trees. For directed graphs, it counts the number of arborescences rooted at a specified node. The function can also consider edge weights to calculate the total weight of all possible trees or arborescences."
    },
    "girth": {
        "technical": "The function returns the girth of a graph, which is the length of the shortest cycle in the graph or infinity if the graph is acyclic. The algorithm's time complexity is O(mn) on a graph with m edges and n nodes.",
        "colloquial": "The function calculates the girth of a graph, which is the length of the shortest cycle in the graph. If the graph has no cycles, it returns infinity. The algorithm used is based on a description from Wikipedia and has a time complexity of O(mn) on a graph with m edges and n nodes."
    },
    "all_pairs_dijkstra": {
        "technical": "Calculate shortest paths and distances between all nodes in a graph using Dijkstra's algorithm. The function allows for specifying a cutoff length for the search and can handle different edge weight representations, either as a string or a custom function. It yields a dictionary for each source node containing distances and paths to all other nodes reachable from that source. The edge weights must be numerical, and distances are computed as the sum of the weights of the edges traversed.",
        "colloquial": "Find the shortest paths and distances between all nodes in a graph. You can set a maximum length for the paths, and specify how edge weights are calculated. The function returns a dictionary for each source node, containing distances and paths to all other nodes reachable from that source."
    },
    "global_efficiency": {
        "technical": "The function calculates the average global efficiency of a graph by computing the efficiency of all pairs of nodes. The efficiency of a pair of nodes is the multiplicative inverse of the shortest path distance between them. Edge weights are not considered in the shortest path distance calculations.",
        "colloquial": "It calculates how well information flows between different points in a network."
    },
    "local_efficiency": {
        "technical": "The function calculates the average local efficiency of a graph by considering the efficiency of each pair of neighboring nodes. Local efficiency of a node is determined by the average global efficiency of its neighboring nodes. The result is the average of these local efficiencies for all nodes in the graph.",
        "colloquial": "This function calculates the average efficiency of information exchange within a node's local neighborhood in a graph. It looks at how well information flows between neighboring nodes in the graph."
    },
    "triadic_census": {
        "technical": "The function determines the triadic census of a directed graph, counting how many of the 16 possible types of triads are present. It takes a directed graph and a list of nodes as input, calculating the triadic census based on the nodes provided. The output is a dictionary where each triad type corresponds to the number of occurrences in the graph. The algorithm's complexity is O(m), where m is the number of edges in the graph. For undirected graphs, the graph can be converted to a directed graph to compute the triadic census.",
        "colloquial": "This function counts the different types of triads present in a directed graph. It gives you a breakdown of how many of each type of triad is in the graph based on the nodes you specify. The output is a dictionary showing the triad type and the number of occurrences for each type."
    },
    "greedy_modularity_communities": {
        "technical": "This function uses Clauset-Newman-Moore greedy modularity maximization to find the community partition with the largest modularity in a graph. It starts with each node in its own community and iteratively merges pairs of communities to maximize modularity until no further increase is possible. The function has two keyword arguments, `cutoff` and `best_n`, to adjust the stopping conditions. The `cutoff` sets a lower limit on the number of communities to stop the process early, while `best_n` sets an upper limit on the number of communities to continue the process even if the maximum modularity occurs for more communities. The resolution parameter `gamma` influences the community sizes favored by the modularity calculation.",
        "colloquial": "This function finds communities in a graph by greedily maximizing modularity. It starts with each node in its own community and merges pairs of communities to increase modularity until no further increase is possible. You can set a minimum and maximum number of communities to control the process. The resolution parameter adjusts the community size preference. The output is a list of communities sorted by size."
    },
    "k_shell": {
        "technical": "The function returns the k-shell of a given graph G, which is the subgraph formed by nodes with a core number of k. It excludes nodes in the (k+1)-core. The parameter k specifies the order of the shell, with the default being the outer shell. It requires core numbers for the graph G. The function does not support MultiGraph objects in version 3.5. It is not implemented for multigraphs or graphs with self loops. The k-shell is similar to the k-corona, but in this case, only neighbors in the k-core are considered. For directed graphs, the node degree is the sum of in-degree and out-degree. Graph, node, and edge attributes are copied to the subgraph.",
        "colloquial": "This function returns a specific part of a graph called the k-shell, which consists of nodes with a certain level of connectivity. It focuses on nodes that form a core at that specific level and are not part of the next higher core. The function does not support MultiGraph objects in the upcoming version."
    },
    "percolation_centrality": {
        "technical": "The function calculates the percolation centrality for nodes in a graph. Percolation centrality for a node is the proportion of 'percolated paths' that pass through that node at a specific time. This metric evaluates the impact of nodes based on their connectivity and percolation states, which represent different scenarios of network percolation over time. Percolation states are typically represented as decimal values between 0.0 and 1.0. When all nodes have the same percolation state, this measure is equivalent to betweenness centrality. The function takes parameters such as the graph, node attribute for percolation state, predefined percolation states, and edge weights. The result is a dictionary mapping nodes to their percolation centrality values.",
        "colloquial": "To put it simply, percolation centrality measures how important a node is in spreading processes. It looks at how many paths that carry influence or information pass through a node, considering both its connections in the network and its current state of influence. This helps us understand which nodes play a significant role in spreading things like infections in a social network, computer viruses in a network, or diseases in a town network. The centrality value ranges from 0 to 1, with higher values indicating greater importance."
    },
    "closeness_vitality": {
        "technical": "The function calculates the closeness vitality for nodes in a graph by measuring the change in the sum of distances between all node pairs when a specific node is removed. It takes into account the graph structure and edge weights if provided, returning either a dictionary mapping each node to its closeness vitality or the closeness vitality for a specified node. The closeness vitality can be negative infinity if removing a node would disconnect the graph.",
        "colloquial": "This function calculates how important each node is in a graph by measuring how much the total distance between all pairs of nodes changes when that node is removed. It gives you a number that shows how vital each node is in keeping the graph connected."
    },
    "edge_current_flow_betweenness_centrality": {
        "technical": "This function computes the current-flow betweenness centrality for edges in a graph. It utilizes an electrical current model for information spreading, unlike traditional betweenness centrality which relies on shortest paths. The function allows for normalization of betweenness values and the specification of edge weights. It also provides options for choosing the data type, solver type, and returns a dictionary of edge tuples with their corresponding current-flow betweenness centrality values.",
        "colloquial": "To put it simply, this function calculates a type of centrality called current-flow betweenness for edges in a graph. It models information spreading like an electrical current, unlike traditional betweenness centrality which focuses on shortest paths. The function can normalize the values and consider edge weights if provided. It uses a linear solver to compute the flow matrix efficiently. The result is a dictionary showing the centrality values for each edge."
    },
    "algebraic_connectivity": {
        "technical": "The function calculates the algebraic connectivity of a connected undirected graph by computing the second smallest eigenvalue of its Laplacian matrix. It takes parameters such as the graph, edge weights, whether to use normalized Laplacian matrix, tolerance for eigenvalue computation, and the method of computation. The function returns the algebraic connectivity value, which indicates the graph's connectivity. It raises exceptions for directed graphs or graphs with less than two nodes. Edge weights are considered as absolute values, and for MultiGraphs, weights of parallel edges are summed. Zero-weighted edges are disregarded.",
        "colloquial": "This function calculates the algebraic connectivity of an undirected graph, which is a measure of how connected the graph is. It uses the second smallest eigenvalue of the Laplacian matrix to determine this connectivity. The function takes parameters like the graph itself, edge weights, and the method for eigenvalue computation. The result is a numerical value representing the algebraic connectivity of the graph."
    },
    "node_connectivity": {
        "technical": "Node connectivity is equal to the minimum number of nodes that must be removed to disconnect G or render it trivial. If source and target nodes are provided, this function returns the local node connectivity: the minimum number of nodes that must be removed to break all paths from source to target in G.",
        "colloquial": "node connectivity measures how 'tough' a graph is. It tells you the minimum number of nodes you would need to remove to break the graph into disconnected parts or make it trivial. If you want to see how easy it is to cut off the flow between two specific points, node connectivity also shows you how many nodes need to be taken out to stop all paths between them."
    }
}
