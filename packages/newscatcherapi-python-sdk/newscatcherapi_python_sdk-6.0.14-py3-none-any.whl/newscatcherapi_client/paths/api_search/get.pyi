# coding: utf-8

"""
    NewsCatcher-V3 Production API

    <img src='https://uploads-ssl.webflow.com/6429857b17973b636c2195c5/646c6f1eb774ff2f2997bec5_newscatcher_.svg' width='286' height='35' /> <br>  <br>Visit our website  <a href='https://newscatcherapi.com'>https://newscatcherapi.com</a>

    The version of the OpenAPI document: 3.2.16
    Contact: maksym@newscatcherapi.com
    Generated by: https://konfigthis.com
"""

from dataclasses import dataclass
import typing_extensions
import urllib3
from pydantic import RootModel
from newscatcherapi_client.request_before_hook import request_before_hook
import json
from urllib3._collections import HTTPHeaderDict

from newscatcherapi_client.api_response import AsyncGeneratorResponse
from newscatcherapi_client import api_client, exceptions
from datetime import date, datetime  # noqa: F401
import decimal  # noqa: F401
import functools  # noqa: F401
import io  # noqa: F401
import re  # noqa: F401
import typing  # noqa: F401
import typing_extensions  # noqa: F401
import uuid  # noqa: F401

import frozendict  # noqa: F401

from newscatcherapi_client import schemas  # noqa: F401

from newscatcherapi_client.model.http_validation_error import HTTPValidationError as HTTPValidationErrorSchema
from newscatcherapi_client.model.search_get_response import SearchGetResponse as SearchGetResponseSchema

from newscatcherapi_client.type.http_validation_error import HTTPValidationError
from newscatcherapi_client.type.search_get_response import SearchGetResponse

from ...api_client import Dictionary
from newscatcherapi_client.pydantic.http_validation_error import HTTPValidationError as HTTPValidationErrorPydantic
from newscatcherapi_client.pydantic.search_get_response import SearchGetResponse as SearchGetResponsePydantic

# Query params
QSchema = schemas.StrSchema
SearchInSchema = schemas.StrSchema
PredefinedSourcesSchema = schemas.AnyTypeSchema
SourcesSchema = schemas.AnyTypeSchema
NotSourcesSchema = schemas.AnyTypeSchema
LangSchema = schemas.AnyTypeSchema
NotLangSchema = schemas.AnyTypeSchema
CountriesSchema = schemas.AnyTypeSchema
NotCountriesSchema = schemas.AnyTypeSchema
NotAuthorNameSchema = schemas.AnyTypeSchema


class ModelFromSchema(
    schemas.ComposedSchema,
):


    class MetaOapg:
        items = schemas.StrSchema
        any_of_1 = schemas.DateTimeSchema
        
        @classmethod
        @functools.lru_cache()
        def any_of(cls):
            # we need this here to make our import statements work
            # we must store _composed_schemas in here so the code is only run
            # when we invoke this method. If we kept this at the class
            # level we would get an error because the class level
            # code would be run when this module is imported, and these composed
            # classes don't exist yet because their module has not finished
            # loading
            return [
                cls.items,
                cls.any_of_1,
            ]


    def __new__(
        cls,
        *args: typing.Union[dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        _configuration: typing.Optional[schemas.Configuration] = None,
        **kwargs: typing.Union[schemas.AnyTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, None, list, tuple, bytes],
    ) -> 'ModelFromSchema':
        return super().__new__(
            cls,
            *args,
            _configuration=_configuration,
            **kwargs,
        )


class ToSchema(
    schemas.ComposedSchema,
):


    class MetaOapg:
        items = schemas.StrSchema
        any_of_1 = schemas.DateTimeSchema
        
        @classmethod
        @functools.lru_cache()
        def any_of(cls):
            # we need this here to make our import statements work
            # we must store _composed_schemas in here so the code is only run
            # when we invoke this method. If we kept this at the class
            # level we would get an error because the class level
            # code would be run when this module is imported, and these composed
            # classes don't exist yet because their module has not finished
            # loading
            return [
                cls.items,
                cls.any_of_1,
            ]


    def __new__(
        cls,
        *args: typing.Union[dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        _configuration: typing.Optional[schemas.Configuration] = None,
        **kwargs: typing.Union[schemas.AnyTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, None, list, tuple, bytes],
    ) -> 'ToSchema':
        return super().__new__(
            cls,
            *args,
            _configuration=_configuration,
            **kwargs,
        )
PublishedDatePrecisionSchema = schemas.StrSchema
ByParseDateSchema = schemas.BoolSchema
SortBySchema = schemas.StrSchema


class RankedOnlySchema(
    schemas.ComposedSchema,
):


    class MetaOapg:
        items = schemas.StrSchema
        any_of_1 = schemas.BoolSchema
        
        @classmethod
        @functools.lru_cache()
        def any_of(cls):
            # we need this here to make our import statements work
            # we must store _composed_schemas in here so the code is only run
            # when we invoke this method. If we kept this at the class
            # level we would get an error because the class level
            # code would be run when this module is imported, and these composed
            # classes don't exist yet because their module has not finished
            # loading
            return [
                cls.items,
                cls.any_of_1,
            ]


    def __new__(
        cls,
        *args: typing.Union[dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        _configuration: typing.Optional[schemas.Configuration] = None,
        **kwargs: typing.Union[schemas.AnyTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, None, list, tuple, bytes],
    ) -> 'RankedOnlySchema':
        return super().__new__(
            cls,
            *args,
            _configuration=_configuration,
            **kwargs,
        )
FromRankSchema = schemas.IntSchema
ToRankSchema = schemas.IntSchema
IsHeadlineSchema = schemas.BoolSchema
IsOpinionSchema = schemas.BoolSchema
IsPaidContentSchema = schemas.BoolSchema
ParentUrlSchema = schemas.AnyTypeSchema
AllLinksSchema = schemas.AnyTypeSchema
AllDomainLinksSchema = schemas.AnyTypeSchema


class WordCountMinSchema(
    schemas.IntSchema
):
    pass


class WordCountMaxSchema(
    schemas.IntSchema
):
    pass


class PageSchema(
    schemas.IntSchema
):
    pass


class PageSizeSchema(
    schemas.IntSchema
):
    pass
ClusteringVariableSchema = schemas.StrSchema
ClusteringEnabledSchema = schemas.BoolSchema
ClusteringThresholdSchema = schemas.NumberSchema
IncludeNlpDataSchema = schemas.BoolSchema
HasNlpSchema = schemas.BoolSchema
ThemeSchema = schemas.StrSchema
NotThemeSchema = schemas.StrSchema
ORGEntityNameSchema = schemas.StrSchema
PEREntityNameSchema = schemas.StrSchema
LOCEntityNameSchema = schemas.StrSchema
MISCEntityNameSchema = schemas.StrSchema
TitleSentimentMinSchema = schemas.NumberSchema
TitleSentimentMaxSchema = schemas.NumberSchema
ContentSentimentMinSchema = schemas.NumberSchema
ContentSentimentMaxSchema = schemas.NumberSchema
IptcTagsSchema = schemas.AnyTypeSchema
NotIptcTagsSchema = schemas.AnyTypeSchema
SourceNameSchema = schemas.AnyTypeSchema
IabTagsSchema = schemas.AnyTypeSchema
NotIabTagsSchema = schemas.AnyTypeSchema
ExcludeDuplicatesSchema = schemas.BoolSchema
AdditionalDomainInfoSchema = schemas.BoolSchema
IsNewsDomainSchema = schemas.BoolSchema
NewsDomainTypeSchema = schemas.AnyTypeSchema
NewsTypeSchema = schemas.AnyTypeSchema
RequestRequiredQueryParams = typing_extensions.TypedDict(
    'RequestRequiredQueryParams',
    {
        'q': typing.Union[QSchema, str, ],
    }
)
RequestOptionalQueryParams = typing_extensions.TypedDict(
    'RequestOptionalQueryParams',
    {
        'search_in': typing.Union[SearchInSchema, str, ],
        'predefined_sources': typing.Union[PredefinedSourcesSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'sources': typing.Union[SourcesSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'not_sources': typing.Union[NotSourcesSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'lang': typing.Union[LangSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'not_lang': typing.Union[NotLangSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'countries': typing.Union[CountriesSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'not_countries': typing.Union[NotCountriesSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'not_author_name': typing.Union[NotAuthorNameSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'from_': typing.Union[ModelFromSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'to_': typing.Union[ToSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'published_date_precision': typing.Union[PublishedDatePrecisionSchema, str, ],
        'by_parse_date': typing.Union[ByParseDateSchema, bool, ],
        'sort_by': typing.Union[SortBySchema, str, ],
        'ranked_only': typing.Union[RankedOnlySchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'from_rank': typing.Union[FromRankSchema, decimal.Decimal, int, ],
        'to_rank': typing.Union[ToRankSchema, decimal.Decimal, int, ],
        'is_headline': typing.Union[IsHeadlineSchema, bool, ],
        'is_opinion': typing.Union[IsOpinionSchema, bool, ],
        'is_paid_content': typing.Union[IsPaidContentSchema, bool, ],
        'parent_url': typing.Union[ParentUrlSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'all_links': typing.Union[AllLinksSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'all_domain_links': typing.Union[AllDomainLinksSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'word_count_min': typing.Union[WordCountMinSchema, decimal.Decimal, int, ],
        'word_count_max': typing.Union[WordCountMaxSchema, decimal.Decimal, int, ],
        'page': typing.Union[PageSchema, decimal.Decimal, int, ],
        'page_size': typing.Union[PageSizeSchema, decimal.Decimal, int, ],
        'clustering_variable': typing.Union[ClusteringVariableSchema, str, ],
        'clustering_enabled': typing.Union[ClusteringEnabledSchema, bool, ],
        'clustering_threshold': typing.Union[ClusteringThresholdSchema, decimal.Decimal, int, float, ],
        'include_nlp_data': typing.Union[IncludeNlpDataSchema, bool, ],
        'has_nlp': typing.Union[HasNlpSchema, bool, ],
        'theme': typing.Union[ThemeSchema, str, ],
        'not_theme': typing.Union[NotThemeSchema, str, ],
        'ORG_entity_name': typing.Union[ORGEntityNameSchema, str, ],
        'PER_entity_name': typing.Union[PEREntityNameSchema, str, ],
        'LOC_entity_name': typing.Union[LOCEntityNameSchema, str, ],
        'MISC_entity_name': typing.Union[MISCEntityNameSchema, str, ],
        'title_sentiment_min': typing.Union[TitleSentimentMinSchema, decimal.Decimal, int, float, ],
        'title_sentiment_max': typing.Union[TitleSentimentMaxSchema, decimal.Decimal, int, float, ],
        'content_sentiment_min': typing.Union[ContentSentimentMinSchema, decimal.Decimal, int, float, ],
        'content_sentiment_max': typing.Union[ContentSentimentMaxSchema, decimal.Decimal, int, float, ],
        'iptc_tags': typing.Union[IptcTagsSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'not_iptc_tags': typing.Union[NotIptcTagsSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'source_name': typing.Union[SourceNameSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'iab_tags': typing.Union[IabTagsSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'not_iab_tags': typing.Union[NotIabTagsSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'exclude_duplicates': typing.Union[ExcludeDuplicatesSchema, bool, ],
        'additional_domain_info': typing.Union[AdditionalDomainInfoSchema, bool, ],
        'is_news_domain': typing.Union[IsNewsDomainSchema, bool, ],
        'news_domain_type': typing.Union[NewsDomainTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
        'news_type': typing.Union[NewsTypeSchema, dict, frozendict.frozendict, str, date, datetime, uuid.UUID, int, float, decimal.Decimal, bool, None, list, tuple, bytes, io.FileIO, io.BufferedReader, ],
    },
    total=False
)


class RequestQueryParams(RequestRequiredQueryParams, RequestOptionalQueryParams):
    pass


request_query_q = api_client.QueryParameter(
    name="q",
    style=api_client.ParameterStyle.FORM,
    schema=QSchema,
    required=True,
    explode=True,
)
request_query_search_in = api_client.QueryParameter(
    name="search_in",
    style=api_client.ParameterStyle.FORM,
    schema=SearchInSchema,
    explode=True,
)
request_query_predefined_sources = api_client.QueryParameter(
    name="predefined_sources",
    style=api_client.ParameterStyle.FORM,
    schema=PredefinedSourcesSchema,
    explode=True,
)
request_query_sources = api_client.QueryParameter(
    name="sources",
    style=api_client.ParameterStyle.FORM,
    schema=SourcesSchema,
    explode=True,
)
request_query_not_sources = api_client.QueryParameter(
    name="not_sources",
    style=api_client.ParameterStyle.FORM,
    schema=NotSourcesSchema,
    explode=True,
)
request_query_lang = api_client.QueryParameter(
    name="lang",
    style=api_client.ParameterStyle.FORM,
    schema=LangSchema,
    explode=True,
)
request_query_not_lang = api_client.QueryParameter(
    name="not_lang",
    style=api_client.ParameterStyle.FORM,
    schema=NotLangSchema,
    explode=True,
)
request_query_countries = api_client.QueryParameter(
    name="countries",
    style=api_client.ParameterStyle.FORM,
    schema=CountriesSchema,
    explode=True,
)
request_query_not_countries = api_client.QueryParameter(
    name="not_countries",
    style=api_client.ParameterStyle.FORM,
    schema=NotCountriesSchema,
    explode=True,
)
request_query_not_author_name = api_client.QueryParameter(
    name="not_author_name",
    style=api_client.ParameterStyle.FORM,
    schema=NotAuthorNameSchema,
    explode=True,
)
request_query_from_ = api_client.QueryParameter(
    name="from_",
    style=api_client.ParameterStyle.FORM,
    schema=ModelFromSchema,
    explode=True,
)
request_query_to_ = api_client.QueryParameter(
    name="to_",
    style=api_client.ParameterStyle.FORM,
    schema=ToSchema,
    explode=True,
)
request_query_published_date_precision = api_client.QueryParameter(
    name="published_date_precision",
    style=api_client.ParameterStyle.FORM,
    schema=PublishedDatePrecisionSchema,
    explode=True,
)
request_query_by_parse_date = api_client.QueryParameter(
    name="by_parse_date",
    style=api_client.ParameterStyle.FORM,
    schema=ByParseDateSchema,
    explode=True,
)
request_query_sort_by = api_client.QueryParameter(
    name="sort_by",
    style=api_client.ParameterStyle.FORM,
    schema=SortBySchema,
    explode=True,
)
request_query_ranked_only = api_client.QueryParameter(
    name="ranked_only",
    style=api_client.ParameterStyle.FORM,
    schema=RankedOnlySchema,
    explode=True,
)
request_query_from_rank = api_client.QueryParameter(
    name="from_rank",
    style=api_client.ParameterStyle.FORM,
    schema=FromRankSchema,
    explode=True,
)
request_query_to_rank = api_client.QueryParameter(
    name="to_rank",
    style=api_client.ParameterStyle.FORM,
    schema=ToRankSchema,
    explode=True,
)
request_query_is_headline = api_client.QueryParameter(
    name="is_headline",
    style=api_client.ParameterStyle.FORM,
    schema=IsHeadlineSchema,
    explode=True,
)
request_query_is_opinion = api_client.QueryParameter(
    name="is_opinion",
    style=api_client.ParameterStyle.FORM,
    schema=IsOpinionSchema,
    explode=True,
)
request_query_is_paid_content = api_client.QueryParameter(
    name="is_paid_content",
    style=api_client.ParameterStyle.FORM,
    schema=IsPaidContentSchema,
    explode=True,
)
request_query_parent_url = api_client.QueryParameter(
    name="parent_url",
    style=api_client.ParameterStyle.FORM,
    schema=ParentUrlSchema,
    explode=True,
)
request_query_all_links = api_client.QueryParameter(
    name="all_links",
    style=api_client.ParameterStyle.FORM,
    schema=AllLinksSchema,
    explode=True,
)
request_query_all_domain_links = api_client.QueryParameter(
    name="all_domain_links",
    style=api_client.ParameterStyle.FORM,
    schema=AllDomainLinksSchema,
    explode=True,
)
request_query_word_count_min = api_client.QueryParameter(
    name="word_count_min",
    style=api_client.ParameterStyle.FORM,
    schema=WordCountMinSchema,
    explode=True,
)
request_query_word_count_max = api_client.QueryParameter(
    name="word_count_max",
    style=api_client.ParameterStyle.FORM,
    schema=WordCountMaxSchema,
    explode=True,
)
request_query_page = api_client.QueryParameter(
    name="page",
    style=api_client.ParameterStyle.FORM,
    schema=PageSchema,
    explode=True,
)
request_query_page_size = api_client.QueryParameter(
    name="page_size",
    style=api_client.ParameterStyle.FORM,
    schema=PageSizeSchema,
    explode=True,
)
request_query_clustering_variable = api_client.QueryParameter(
    name="clustering_variable",
    style=api_client.ParameterStyle.FORM,
    schema=ClusteringVariableSchema,
    explode=True,
)
request_query_clustering_enabled = api_client.QueryParameter(
    name="clustering_enabled",
    style=api_client.ParameterStyle.FORM,
    schema=ClusteringEnabledSchema,
    explode=True,
)
request_query_clustering_threshold = api_client.QueryParameter(
    name="clustering_threshold",
    style=api_client.ParameterStyle.FORM,
    schema=ClusteringThresholdSchema,
    explode=True,
)
request_query_include_nlp_data = api_client.QueryParameter(
    name="include_nlp_data",
    style=api_client.ParameterStyle.FORM,
    schema=IncludeNlpDataSchema,
    explode=True,
)
request_query_has_nlp = api_client.QueryParameter(
    name="has_nlp",
    style=api_client.ParameterStyle.FORM,
    schema=HasNlpSchema,
    explode=True,
)
request_query_theme = api_client.QueryParameter(
    name="theme",
    style=api_client.ParameterStyle.FORM,
    schema=ThemeSchema,
    explode=True,
)
request_query_not_theme = api_client.QueryParameter(
    name="not_theme",
    style=api_client.ParameterStyle.FORM,
    schema=NotThemeSchema,
    explode=True,
)
request_query_org_entity_name = api_client.QueryParameter(
    name="ORG_entity_name",
    style=api_client.ParameterStyle.FORM,
    schema=ORGEntityNameSchema,
    explode=True,
)
request_query_per_entity_name = api_client.QueryParameter(
    name="PER_entity_name",
    style=api_client.ParameterStyle.FORM,
    schema=PEREntityNameSchema,
    explode=True,
)
request_query_loc_entity_name = api_client.QueryParameter(
    name="LOC_entity_name",
    style=api_client.ParameterStyle.FORM,
    schema=LOCEntityNameSchema,
    explode=True,
)
request_query_misc_entity_name = api_client.QueryParameter(
    name="MISC_entity_name",
    style=api_client.ParameterStyle.FORM,
    schema=MISCEntityNameSchema,
    explode=True,
)
request_query_title_sentiment_min = api_client.QueryParameter(
    name="title_sentiment_min",
    style=api_client.ParameterStyle.FORM,
    schema=TitleSentimentMinSchema,
    explode=True,
)
request_query_title_sentiment_max = api_client.QueryParameter(
    name="title_sentiment_max",
    style=api_client.ParameterStyle.FORM,
    schema=TitleSentimentMaxSchema,
    explode=True,
)
request_query_content_sentiment_min = api_client.QueryParameter(
    name="content_sentiment_min",
    style=api_client.ParameterStyle.FORM,
    schema=ContentSentimentMinSchema,
    explode=True,
)
request_query_content_sentiment_max = api_client.QueryParameter(
    name="content_sentiment_max",
    style=api_client.ParameterStyle.FORM,
    schema=ContentSentimentMaxSchema,
    explode=True,
)
request_query_iptc_tags = api_client.QueryParameter(
    name="iptc_tags",
    style=api_client.ParameterStyle.FORM,
    schema=IptcTagsSchema,
    explode=True,
)
request_query_not_iptc_tags = api_client.QueryParameter(
    name="not_iptc_tags",
    style=api_client.ParameterStyle.FORM,
    schema=NotIptcTagsSchema,
    explode=True,
)
request_query_source_name = api_client.QueryParameter(
    name="source_name",
    style=api_client.ParameterStyle.FORM,
    schema=SourceNameSchema,
    explode=True,
)
request_query_iab_tags = api_client.QueryParameter(
    name="iab_tags",
    style=api_client.ParameterStyle.FORM,
    schema=IabTagsSchema,
    explode=True,
)
request_query_not_iab_tags = api_client.QueryParameter(
    name="not_iab_tags",
    style=api_client.ParameterStyle.FORM,
    schema=NotIabTagsSchema,
    explode=True,
)
request_query_exclude_duplicates = api_client.QueryParameter(
    name="exclude_duplicates",
    style=api_client.ParameterStyle.FORM,
    schema=ExcludeDuplicatesSchema,
    explode=True,
)
request_query_additional_domain_info = api_client.QueryParameter(
    name="additional_domain_info",
    style=api_client.ParameterStyle.FORM,
    schema=AdditionalDomainInfoSchema,
    explode=True,
)
request_query_is_news_domain = api_client.QueryParameter(
    name="is_news_domain",
    style=api_client.ParameterStyle.FORM,
    schema=IsNewsDomainSchema,
    explode=True,
)
request_query_news_domain_type = api_client.QueryParameter(
    name="news_domain_type",
    style=api_client.ParameterStyle.FORM,
    schema=NewsDomainTypeSchema,
    explode=True,
)
request_query_news_type = api_client.QueryParameter(
    name="news_type",
    style=api_client.ParameterStyle.FORM,
    schema=NewsTypeSchema,
    explode=True,
)
SchemaFor200ResponseBodyApplicationJson = SearchGetResponseSchema


@dataclass
class ApiResponseFor200(api_client.ApiResponse):
    body: SearchGetResponse


@dataclass
class ApiResponseFor200Async(api_client.AsyncApiResponse):
    body: SearchGetResponse


_response_for_200 = api_client.OpenApiResponse(
    response_cls=ApiResponseFor200,
    response_cls_async=ApiResponseFor200Async,
    content={
        'application/json': api_client.MediaType(
            schema=SchemaFor200ResponseBodyApplicationJson),
    },
)
SchemaFor422ResponseBodyApplicationJson = HTTPValidationErrorSchema


@dataclass
class ApiResponseFor422(api_client.ApiResponse):
    body: HTTPValidationError


@dataclass
class ApiResponseFor422Async(api_client.AsyncApiResponse):
    body: HTTPValidationError


_response_for_422 = api_client.OpenApiResponse(
    response_cls=ApiResponseFor422,
    response_cls_async=ApiResponseFor422Async,
    content={
        'application/json': api_client.MediaType(
            schema=SchemaFor422ResponseBodyApplicationJson),
    },
)
_all_accept_content_types = (
    'application/json',
)


class BaseApi(api_client.Api):

    def _get_mapped_args(
        self,
        q: str,
        search_in: typing.Optional[str] = None,
        predefined_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_author_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        from_: typing.Optional[typing.Union[str, datetime]] = None,
        to_: typing.Optional[typing.Union[str, datetime]] = None,
        published_date_precision: typing.Optional[str] = None,
        by_parse_date: typing.Optional[bool] = None,
        sort_by: typing.Optional[str] = None,
        ranked_only: typing.Optional[typing.Union[str, bool]] = None,
        from_rank: typing.Optional[int] = None,
        to_rank: typing.Optional[int] = None,
        is_headline: typing.Optional[bool] = None,
        is_opinion: typing.Optional[bool] = None,
        is_paid_content: typing.Optional[bool] = None,
        parent_url: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_domain_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        word_count_min: typing.Optional[int] = None,
        word_count_max: typing.Optional[int] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
        clustering_variable: typing.Optional[str] = None,
        clustering_enabled: typing.Optional[bool] = None,
        clustering_threshold: typing.Optional[typing.Union[int, float]] = None,
        include_nlp_data: typing.Optional[bool] = None,
        has_nlp: typing.Optional[bool] = None,
        theme: typing.Optional[str] = None,
        not_theme: typing.Optional[str] = None,
        org_entity_name: typing.Optional[str] = None,
        per_entity_name: typing.Optional[str] = None,
        loc_entity_name: typing.Optional[str] = None,
        misc_entity_name: typing.Optional[str] = None,
        title_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        title_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        source_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        exclude_duplicates: typing.Optional[bool] = None,
        additional_domain_info: typing.Optional[bool] = None,
        is_news_domain: typing.Optional[bool] = None,
        news_domain_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        news_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
    ) -> api_client.MappedArgs:
        args: api_client.MappedArgs = api_client.MappedArgs()
        _query_params = {}
        if q is not None:
            _query_params["q"] = q
        if search_in is not None:
            _query_params["search_in"] = search_in
        if predefined_sources is not None:
            _query_params["predefined_sources"] = predefined_sources
        if sources is not None:
            _query_params["sources"] = sources
        if not_sources is not None:
            _query_params["not_sources"] = not_sources
        if lang is not None:
            _query_params["lang"] = lang
        if not_lang is not None:
            _query_params["not_lang"] = not_lang
        if countries is not None:
            _query_params["countries"] = countries
        if not_countries is not None:
            _query_params["not_countries"] = not_countries
        if not_author_name is not None:
            _query_params["not_author_name"] = not_author_name
        if from_ is not None:
            _query_params["from_"] = from_
        if to_ is not None:
            _query_params["to_"] = to_
        if published_date_precision is not None:
            _query_params["published_date_precision"] = published_date_precision
        if by_parse_date is not None:
            _query_params["by_parse_date"] = by_parse_date
        if sort_by is not None:
            _query_params["sort_by"] = sort_by
        if ranked_only is not None:
            _query_params["ranked_only"] = ranked_only
        if from_rank is not None:
            _query_params["from_rank"] = from_rank
        if to_rank is not None:
            _query_params["to_rank"] = to_rank
        if is_headline is not None:
            _query_params["is_headline"] = is_headline
        if is_opinion is not None:
            _query_params["is_opinion"] = is_opinion
        if is_paid_content is not None:
            _query_params["is_paid_content"] = is_paid_content
        if parent_url is not None:
            _query_params["parent_url"] = parent_url
        if all_links is not None:
            _query_params["all_links"] = all_links
        if all_domain_links is not None:
            _query_params["all_domain_links"] = all_domain_links
        if word_count_min is not None:
            _query_params["word_count_min"] = word_count_min
        if word_count_max is not None:
            _query_params["word_count_max"] = word_count_max
        if page is not None:
            _query_params["page"] = page
        if page_size is not None:
            _query_params["page_size"] = page_size
        if clustering_variable is not None:
            _query_params["clustering_variable"] = clustering_variable
        if clustering_enabled is not None:
            _query_params["clustering_enabled"] = clustering_enabled
        if clustering_threshold is not None:
            _query_params["clustering_threshold"] = clustering_threshold
        if include_nlp_data is not None:
            _query_params["include_nlp_data"] = include_nlp_data
        if has_nlp is not None:
            _query_params["has_nlp"] = has_nlp
        if theme is not None:
            _query_params["theme"] = theme
        if not_theme is not None:
            _query_params["not_theme"] = not_theme
        if org_entity_name is not None:
            _query_params["ORG_entity_name"] = org_entity_name
        if per_entity_name is not None:
            _query_params["PER_entity_name"] = per_entity_name
        if loc_entity_name is not None:
            _query_params["LOC_entity_name"] = loc_entity_name
        if misc_entity_name is not None:
            _query_params["MISC_entity_name"] = misc_entity_name
        if title_sentiment_min is not None:
            _query_params["title_sentiment_min"] = title_sentiment_min
        if title_sentiment_max is not None:
            _query_params["title_sentiment_max"] = title_sentiment_max
        if content_sentiment_min is not None:
            _query_params["content_sentiment_min"] = content_sentiment_min
        if content_sentiment_max is not None:
            _query_params["content_sentiment_max"] = content_sentiment_max
        if iptc_tags is not None:
            _query_params["iptc_tags"] = iptc_tags
        if not_iptc_tags is not None:
            _query_params["not_iptc_tags"] = not_iptc_tags
        if source_name is not None:
            _query_params["source_name"] = source_name
        if iab_tags is not None:
            _query_params["iab_tags"] = iab_tags
        if not_iab_tags is not None:
            _query_params["not_iab_tags"] = not_iab_tags
        if exclude_duplicates is not None:
            _query_params["exclude_duplicates"] = exclude_duplicates
        if additional_domain_info is not None:
            _query_params["additional_domain_info"] = additional_domain_info
        if is_news_domain is not None:
            _query_params["is_news_domain"] = is_news_domain
        if news_domain_type is not None:
            _query_params["news_domain_type"] = news_domain_type
        if news_type is not None:
            _query_params["news_type"] = news_type
        args.query = _query_params
        return args

    async def _aget_oapg(
        self,
            query_params: typing.Optional[dict] = {},
        skip_deserialization: bool = True,
        timeout: typing.Optional[typing.Union[float, typing.Tuple]] = None,
        accept_content_types: typing.Tuple[str] = _all_accept_content_types,
        stream: bool = False,
        **kwargs,
    ) -> typing.Union[
        ApiResponseFor200Async,
        api_client.ApiResponseWithoutDeserializationAsync,
        AsyncGeneratorResponse,
    ]:
        """
        [Get] Search For Articles Request
        :param skip_deserialization: If true then api_response.response will be set but
            api_response.body and api_response.headers will not be deserialized into schema
            class instances
        """
        self._verify_typed_dict_inputs_oapg(RequestQueryParams, query_params)
        used_path = path.value
    
        prefix_separator_iterator = None
        for parameter in (
            request_query_q,
            request_query_search_in,
            request_query_predefined_sources,
            request_query_sources,
            request_query_not_sources,
            request_query_lang,
            request_query_not_lang,
            request_query_countries,
            request_query_not_countries,
            request_query_not_author_name,
            request_query_from_,
            request_query_to_,
            request_query_published_date_precision,
            request_query_by_parse_date,
            request_query_sort_by,
            request_query_ranked_only,
            request_query_from_rank,
            request_query_to_rank,
            request_query_is_headline,
            request_query_is_opinion,
            request_query_is_paid_content,
            request_query_parent_url,
            request_query_all_links,
            request_query_all_domain_links,
            request_query_word_count_min,
            request_query_word_count_max,
            request_query_page,
            request_query_page_size,
            request_query_clustering_variable,
            request_query_clustering_enabled,
            request_query_clustering_threshold,
            request_query_include_nlp_data,
            request_query_has_nlp,
            request_query_theme,
            request_query_not_theme,
            request_query_org_entity_name,
            request_query_per_entity_name,
            request_query_loc_entity_name,
            request_query_misc_entity_name,
            request_query_title_sentiment_min,
            request_query_title_sentiment_max,
            request_query_content_sentiment_min,
            request_query_content_sentiment_max,
            request_query_iptc_tags,
            request_query_not_iptc_tags,
            request_query_source_name,
            request_query_iab_tags,
            request_query_not_iab_tags,
            request_query_exclude_duplicates,
            request_query_additional_domain_info,
            request_query_is_news_domain,
            request_query_news_domain_type,
            request_query_news_type,
        ):
            parameter_data = query_params.get(parameter.name, schemas.unset)
            if parameter_data is schemas.unset:
                continue
            if prefix_separator_iterator is None:
                prefix_separator_iterator = parameter.get_prefix_separator_iterator()
            serialized_data = parameter.serialize(parameter_data, prefix_separator_iterator)
            for serialized_value in serialized_data.values():
                used_path += serialized_value
    
        _headers = HTTPHeaderDict()
        # TODO add cookie handling
        if accept_content_types:
            for accept_content_type in accept_content_types:
                _headers.add('Accept', accept_content_type)
        method = 'get'.upper()
        request_before_hook(
            resource_path=used_path,
            method=method,
            configuration=self.api_client.configuration,
            path_template='/api/search',
            auth_settings=_auth,
            headers=_headers,
        )
    
        response = await self.api_client.async_call_api(
            resource_path=used_path,
            method=method,
            headers=_headers,
            auth_settings=_auth,
            prefix_separator_iterator=prefix_separator_iterator,
            timeout=timeout,
            **kwargs
        )
    
        if stream:
            if not 200 <= response.http_response.status <= 299:
                body = (await response.http_response.content.read()).decode("utf-8")
                raise exceptions.ApiStreamingException(
                    status=response.http_response.status,
                    reason=response.http_response.reason,
                    body=body,
                )
    
            async def stream_iterator():
                """
                iterates over response.http_response.content and closes connection once iteration has finished
                """
                async for line in response.http_response.content:
                    if line == b'\r\n':
                        continue
                    yield line
                response.http_response.close()
                await response.session.close()
            return AsyncGeneratorResponse(
                content=stream_iterator(),
                headers=response.http_response.headers,
                status=response.http_response.status,
                response=response.http_response
            )
    
        response_for_status = _status_code_to_response.get(str(response.http_response.status))
        if response_for_status:
            api_response = await response_for_status.deserialize_async(
                                                    response,
                                                    self.api_client.configuration,
                                                    skip_deserialization=skip_deserialization
                                                )
        else:
            # If response data is JSON then deserialize for SDK consumer convenience
            is_json = api_client.JSONDetector._content_type_is_json(response.http_response.headers.get('Content-Type', ''))
            api_response = api_client.ApiResponseWithoutDeserializationAsync(
                body=await response.http_response.json() if is_json else await response.http_response.text(),
                response=response.http_response,
                round_trip_time=response.round_trip_time,
                status=response.http_response.status,
                headers=response.http_response.headers,
            )
    
        if not 200 <= api_response.status <= 299:
            raise exceptions.ApiException(api_response=api_response)
    
        # cleanup session / response
        response.http_response.close()
        await response.session.close()
    
        return api_response


    def _get_oapg(
        self,
            query_params: typing.Optional[dict] = {},
        skip_deserialization: bool = True,
        timeout: typing.Optional[typing.Union[float, typing.Tuple]] = None,
        accept_content_types: typing.Tuple[str] = _all_accept_content_types,
        stream: bool = False,
    ) -> typing.Union[
        ApiResponseFor200,
        api_client.ApiResponseWithoutDeserialization,
    ]:
        """
        [Get] Search For Articles Request
        :param skip_deserialization: If true then api_response.response will be set but
            api_response.body and api_response.headers will not be deserialized into schema
            class instances
        """
        self._verify_typed_dict_inputs_oapg(RequestQueryParams, query_params)
        used_path = path.value
    
        prefix_separator_iterator = None
        for parameter in (
            request_query_q,
            request_query_search_in,
            request_query_predefined_sources,
            request_query_sources,
            request_query_not_sources,
            request_query_lang,
            request_query_not_lang,
            request_query_countries,
            request_query_not_countries,
            request_query_not_author_name,
            request_query_from_,
            request_query_to_,
            request_query_published_date_precision,
            request_query_by_parse_date,
            request_query_sort_by,
            request_query_ranked_only,
            request_query_from_rank,
            request_query_to_rank,
            request_query_is_headline,
            request_query_is_opinion,
            request_query_is_paid_content,
            request_query_parent_url,
            request_query_all_links,
            request_query_all_domain_links,
            request_query_word_count_min,
            request_query_word_count_max,
            request_query_page,
            request_query_page_size,
            request_query_clustering_variable,
            request_query_clustering_enabled,
            request_query_clustering_threshold,
            request_query_include_nlp_data,
            request_query_has_nlp,
            request_query_theme,
            request_query_not_theme,
            request_query_org_entity_name,
            request_query_per_entity_name,
            request_query_loc_entity_name,
            request_query_misc_entity_name,
            request_query_title_sentiment_min,
            request_query_title_sentiment_max,
            request_query_content_sentiment_min,
            request_query_content_sentiment_max,
            request_query_iptc_tags,
            request_query_not_iptc_tags,
            request_query_source_name,
            request_query_iab_tags,
            request_query_not_iab_tags,
            request_query_exclude_duplicates,
            request_query_additional_domain_info,
            request_query_is_news_domain,
            request_query_news_domain_type,
            request_query_news_type,
        ):
            parameter_data = query_params.get(parameter.name, schemas.unset)
            if parameter_data is schemas.unset:
                continue
            if prefix_separator_iterator is None:
                prefix_separator_iterator = parameter.get_prefix_separator_iterator()
            serialized_data = parameter.serialize(parameter_data, prefix_separator_iterator)
            for serialized_value in serialized_data.values():
                used_path += serialized_value
    
        _headers = HTTPHeaderDict()
        # TODO add cookie handling
        if accept_content_types:
            for accept_content_type in accept_content_types:
                _headers.add('Accept', accept_content_type)
        method = 'get'.upper()
        request_before_hook(
            resource_path=used_path,
            method=method,
            configuration=self.api_client.configuration,
            path_template='/api/search',
            auth_settings=_auth,
            headers=_headers,
        )
    
        response = self.api_client.call_api(
            resource_path=used_path,
            method=method,
            headers=_headers,
            auth_settings=_auth,
            prefix_separator_iterator=prefix_separator_iterator,
            timeout=timeout,
        )
    
        response_for_status = _status_code_to_response.get(str(response.http_response.status))
        if response_for_status:
            api_response = response_for_status.deserialize(
                                                    response,
                                                    self.api_client.configuration,
                                                    skip_deserialization=skip_deserialization
                                                )
        else:
            # If response data is JSON then deserialize for SDK consumer convenience
            is_json = api_client.JSONDetector._content_type_is_json(response.http_response.headers.get('Content-Type', ''))
            api_response = api_client.ApiResponseWithoutDeserialization(
                body=json.loads(response.http_response.data) if is_json else response.http_response.data,
                response=response.http_response,
                round_trip_time=response.round_trip_time,
                status=response.http_response.status,
                headers=response.http_response.headers,
            )
    
        if not 200 <= api_response.status <= 299:
            raise exceptions.ApiException(api_response=api_response)
    
        return api_response


class GetRaw(BaseApi):
    # this class is used by api classes that refer to endpoints with operationId fn names

    async def aget(
        self,
        q: str,
        search_in: typing.Optional[str] = None,
        predefined_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_author_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        from_: typing.Optional[typing.Union[str, datetime]] = None,
        to_: typing.Optional[typing.Union[str, datetime]] = None,
        published_date_precision: typing.Optional[str] = None,
        by_parse_date: typing.Optional[bool] = None,
        sort_by: typing.Optional[str] = None,
        ranked_only: typing.Optional[typing.Union[str, bool]] = None,
        from_rank: typing.Optional[int] = None,
        to_rank: typing.Optional[int] = None,
        is_headline: typing.Optional[bool] = None,
        is_opinion: typing.Optional[bool] = None,
        is_paid_content: typing.Optional[bool] = None,
        parent_url: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_domain_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        word_count_min: typing.Optional[int] = None,
        word_count_max: typing.Optional[int] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
        clustering_variable: typing.Optional[str] = None,
        clustering_enabled: typing.Optional[bool] = None,
        clustering_threshold: typing.Optional[typing.Union[int, float]] = None,
        include_nlp_data: typing.Optional[bool] = None,
        has_nlp: typing.Optional[bool] = None,
        theme: typing.Optional[str] = None,
        not_theme: typing.Optional[str] = None,
        org_entity_name: typing.Optional[str] = None,
        per_entity_name: typing.Optional[str] = None,
        loc_entity_name: typing.Optional[str] = None,
        misc_entity_name: typing.Optional[str] = None,
        title_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        title_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        source_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        exclude_duplicates: typing.Optional[bool] = None,
        additional_domain_info: typing.Optional[bool] = None,
        is_news_domain: typing.Optional[bool] = None,
        news_domain_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        news_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        **kwargs,
    ) -> typing.Union[
        ApiResponseFor200Async,
        api_client.ApiResponseWithoutDeserializationAsync,
        AsyncGeneratorResponse,
    ]:
        args = self._get_mapped_args(
            q=q,
            search_in=search_in,
            predefined_sources=predefined_sources,
            sources=sources,
            not_sources=not_sources,
            lang=lang,
            not_lang=not_lang,
            countries=countries,
            not_countries=not_countries,
            not_author_name=not_author_name,
            from_=from_,
            to_=to_,
            published_date_precision=published_date_precision,
            by_parse_date=by_parse_date,
            sort_by=sort_by,
            ranked_only=ranked_only,
            from_rank=from_rank,
            to_rank=to_rank,
            is_headline=is_headline,
            is_opinion=is_opinion,
            is_paid_content=is_paid_content,
            parent_url=parent_url,
            all_links=all_links,
            all_domain_links=all_domain_links,
            word_count_min=word_count_min,
            word_count_max=word_count_max,
            page=page,
            page_size=page_size,
            clustering_variable=clustering_variable,
            clustering_enabled=clustering_enabled,
            clustering_threshold=clustering_threshold,
            include_nlp_data=include_nlp_data,
            has_nlp=has_nlp,
            theme=theme,
            not_theme=not_theme,
            org_entity_name=org_entity_name,
            per_entity_name=per_entity_name,
            loc_entity_name=loc_entity_name,
            misc_entity_name=misc_entity_name,
            title_sentiment_min=title_sentiment_min,
            title_sentiment_max=title_sentiment_max,
            content_sentiment_min=content_sentiment_min,
            content_sentiment_max=content_sentiment_max,
            iptc_tags=iptc_tags,
            not_iptc_tags=not_iptc_tags,
            source_name=source_name,
            iab_tags=iab_tags,
            not_iab_tags=not_iab_tags,
            exclude_duplicates=exclude_duplicates,
            additional_domain_info=additional_domain_info,
            is_news_domain=is_news_domain,
            news_domain_type=news_domain_type,
            news_type=news_type,
        )
        return await self._aget_oapg(
            query_params=args.query,
            **kwargs,
        )
    
    def get(
        self,
        q: str,
        search_in: typing.Optional[str] = None,
        predefined_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_author_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        from_: typing.Optional[typing.Union[str, datetime]] = None,
        to_: typing.Optional[typing.Union[str, datetime]] = None,
        published_date_precision: typing.Optional[str] = None,
        by_parse_date: typing.Optional[bool] = None,
        sort_by: typing.Optional[str] = None,
        ranked_only: typing.Optional[typing.Union[str, bool]] = None,
        from_rank: typing.Optional[int] = None,
        to_rank: typing.Optional[int] = None,
        is_headline: typing.Optional[bool] = None,
        is_opinion: typing.Optional[bool] = None,
        is_paid_content: typing.Optional[bool] = None,
        parent_url: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_domain_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        word_count_min: typing.Optional[int] = None,
        word_count_max: typing.Optional[int] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
        clustering_variable: typing.Optional[str] = None,
        clustering_enabled: typing.Optional[bool] = None,
        clustering_threshold: typing.Optional[typing.Union[int, float]] = None,
        include_nlp_data: typing.Optional[bool] = None,
        has_nlp: typing.Optional[bool] = None,
        theme: typing.Optional[str] = None,
        not_theme: typing.Optional[str] = None,
        org_entity_name: typing.Optional[str] = None,
        per_entity_name: typing.Optional[str] = None,
        loc_entity_name: typing.Optional[str] = None,
        misc_entity_name: typing.Optional[str] = None,
        title_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        title_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        source_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        exclude_duplicates: typing.Optional[bool] = None,
        additional_domain_info: typing.Optional[bool] = None,
        is_news_domain: typing.Optional[bool] = None,
        news_domain_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        news_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
    ) -> typing.Union[
        ApiResponseFor200,
        api_client.ApiResponseWithoutDeserialization,
    ]:
        """ This endpoint allows you to search for articles. You can search for articles by keyword, language, country, source, and more. """
        args = self._get_mapped_args(
            q=q,
            search_in=search_in,
            predefined_sources=predefined_sources,
            sources=sources,
            not_sources=not_sources,
            lang=lang,
            not_lang=not_lang,
            countries=countries,
            not_countries=not_countries,
            not_author_name=not_author_name,
            from_=from_,
            to_=to_,
            published_date_precision=published_date_precision,
            by_parse_date=by_parse_date,
            sort_by=sort_by,
            ranked_only=ranked_only,
            from_rank=from_rank,
            to_rank=to_rank,
            is_headline=is_headline,
            is_opinion=is_opinion,
            is_paid_content=is_paid_content,
            parent_url=parent_url,
            all_links=all_links,
            all_domain_links=all_domain_links,
            word_count_min=word_count_min,
            word_count_max=word_count_max,
            page=page,
            page_size=page_size,
            clustering_variable=clustering_variable,
            clustering_enabled=clustering_enabled,
            clustering_threshold=clustering_threshold,
            include_nlp_data=include_nlp_data,
            has_nlp=has_nlp,
            theme=theme,
            not_theme=not_theme,
            org_entity_name=org_entity_name,
            per_entity_name=per_entity_name,
            loc_entity_name=loc_entity_name,
            misc_entity_name=misc_entity_name,
            title_sentiment_min=title_sentiment_min,
            title_sentiment_max=title_sentiment_max,
            content_sentiment_min=content_sentiment_min,
            content_sentiment_max=content_sentiment_max,
            iptc_tags=iptc_tags,
            not_iptc_tags=not_iptc_tags,
            source_name=source_name,
            iab_tags=iab_tags,
            not_iab_tags=not_iab_tags,
            exclude_duplicates=exclude_duplicates,
            additional_domain_info=additional_domain_info,
            is_news_domain=is_news_domain,
            news_domain_type=news_domain_type,
            news_type=news_type,
        )
        return self._get_oapg(
            query_params=args.query,
        )

class Get(BaseApi):

    async def aget(
        self,
        q: str,
        search_in: typing.Optional[str] = None,
        predefined_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_author_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        from_: typing.Optional[typing.Union[str, datetime]] = None,
        to_: typing.Optional[typing.Union[str, datetime]] = None,
        published_date_precision: typing.Optional[str] = None,
        by_parse_date: typing.Optional[bool] = None,
        sort_by: typing.Optional[str] = None,
        ranked_only: typing.Optional[typing.Union[str, bool]] = None,
        from_rank: typing.Optional[int] = None,
        to_rank: typing.Optional[int] = None,
        is_headline: typing.Optional[bool] = None,
        is_opinion: typing.Optional[bool] = None,
        is_paid_content: typing.Optional[bool] = None,
        parent_url: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_domain_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        word_count_min: typing.Optional[int] = None,
        word_count_max: typing.Optional[int] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
        clustering_variable: typing.Optional[str] = None,
        clustering_enabled: typing.Optional[bool] = None,
        clustering_threshold: typing.Optional[typing.Union[int, float]] = None,
        include_nlp_data: typing.Optional[bool] = None,
        has_nlp: typing.Optional[bool] = None,
        theme: typing.Optional[str] = None,
        not_theme: typing.Optional[str] = None,
        org_entity_name: typing.Optional[str] = None,
        per_entity_name: typing.Optional[str] = None,
        loc_entity_name: typing.Optional[str] = None,
        misc_entity_name: typing.Optional[str] = None,
        title_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        title_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        source_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        exclude_duplicates: typing.Optional[bool] = None,
        additional_domain_info: typing.Optional[bool] = None,
        is_news_domain: typing.Optional[bool] = None,
        news_domain_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        news_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        validate: bool = False,
        **kwargs,
    ) -> SearchGetResponsePydantic:
        raw_response = await self.raw.aget(
            q=q,
            search_in=search_in,
            predefined_sources=predefined_sources,
            sources=sources,
            not_sources=not_sources,
            lang=lang,
            not_lang=not_lang,
            countries=countries,
            not_countries=not_countries,
            not_author_name=not_author_name,
            from_=from_,
            to_=to_,
            published_date_precision=published_date_precision,
            by_parse_date=by_parse_date,
            sort_by=sort_by,
            ranked_only=ranked_only,
            from_rank=from_rank,
            to_rank=to_rank,
            is_headline=is_headline,
            is_opinion=is_opinion,
            is_paid_content=is_paid_content,
            parent_url=parent_url,
            all_links=all_links,
            all_domain_links=all_domain_links,
            word_count_min=word_count_min,
            word_count_max=word_count_max,
            page=page,
            page_size=page_size,
            clustering_variable=clustering_variable,
            clustering_enabled=clustering_enabled,
            clustering_threshold=clustering_threshold,
            include_nlp_data=include_nlp_data,
            has_nlp=has_nlp,
            theme=theme,
            not_theme=not_theme,
            org_entity_name=org_entity_name,
            per_entity_name=per_entity_name,
            loc_entity_name=loc_entity_name,
            misc_entity_name=misc_entity_name,
            title_sentiment_min=title_sentiment_min,
            title_sentiment_max=title_sentiment_max,
            content_sentiment_min=content_sentiment_min,
            content_sentiment_max=content_sentiment_max,
            iptc_tags=iptc_tags,
            not_iptc_tags=not_iptc_tags,
            source_name=source_name,
            iab_tags=iab_tags,
            not_iab_tags=not_iab_tags,
            exclude_duplicates=exclude_duplicates,
            additional_domain_info=additional_domain_info,
            is_news_domain=is_news_domain,
            news_domain_type=news_domain_type,
            news_type=news_type,
            **kwargs,
        )
        if validate:
            return RootModel[SearchGetResponsePydantic](raw_response.body).root
        return api_client.construct_model_instance(SearchGetResponsePydantic, raw_response.body)
    
    
    def get(
        self,
        q: str,
        search_in: typing.Optional[str] = None,
        predefined_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_author_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        from_: typing.Optional[typing.Union[str, datetime]] = None,
        to_: typing.Optional[typing.Union[str, datetime]] = None,
        published_date_precision: typing.Optional[str] = None,
        by_parse_date: typing.Optional[bool] = None,
        sort_by: typing.Optional[str] = None,
        ranked_only: typing.Optional[typing.Union[str, bool]] = None,
        from_rank: typing.Optional[int] = None,
        to_rank: typing.Optional[int] = None,
        is_headline: typing.Optional[bool] = None,
        is_opinion: typing.Optional[bool] = None,
        is_paid_content: typing.Optional[bool] = None,
        parent_url: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_domain_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        word_count_min: typing.Optional[int] = None,
        word_count_max: typing.Optional[int] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
        clustering_variable: typing.Optional[str] = None,
        clustering_enabled: typing.Optional[bool] = None,
        clustering_threshold: typing.Optional[typing.Union[int, float]] = None,
        include_nlp_data: typing.Optional[bool] = None,
        has_nlp: typing.Optional[bool] = None,
        theme: typing.Optional[str] = None,
        not_theme: typing.Optional[str] = None,
        org_entity_name: typing.Optional[str] = None,
        per_entity_name: typing.Optional[str] = None,
        loc_entity_name: typing.Optional[str] = None,
        misc_entity_name: typing.Optional[str] = None,
        title_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        title_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        source_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        exclude_duplicates: typing.Optional[bool] = None,
        additional_domain_info: typing.Optional[bool] = None,
        is_news_domain: typing.Optional[bool] = None,
        news_domain_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        news_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        validate: bool = False,
    ) -> SearchGetResponsePydantic:
        raw_response = self.raw.get(
            q=q,
            search_in=search_in,
            predefined_sources=predefined_sources,
            sources=sources,
            not_sources=not_sources,
            lang=lang,
            not_lang=not_lang,
            countries=countries,
            not_countries=not_countries,
            not_author_name=not_author_name,
            from_=from_,
            to_=to_,
            published_date_precision=published_date_precision,
            by_parse_date=by_parse_date,
            sort_by=sort_by,
            ranked_only=ranked_only,
            from_rank=from_rank,
            to_rank=to_rank,
            is_headline=is_headline,
            is_opinion=is_opinion,
            is_paid_content=is_paid_content,
            parent_url=parent_url,
            all_links=all_links,
            all_domain_links=all_domain_links,
            word_count_min=word_count_min,
            word_count_max=word_count_max,
            page=page,
            page_size=page_size,
            clustering_variable=clustering_variable,
            clustering_enabled=clustering_enabled,
            clustering_threshold=clustering_threshold,
            include_nlp_data=include_nlp_data,
            has_nlp=has_nlp,
            theme=theme,
            not_theme=not_theme,
            org_entity_name=org_entity_name,
            per_entity_name=per_entity_name,
            loc_entity_name=loc_entity_name,
            misc_entity_name=misc_entity_name,
            title_sentiment_min=title_sentiment_min,
            title_sentiment_max=title_sentiment_max,
            content_sentiment_min=content_sentiment_min,
            content_sentiment_max=content_sentiment_max,
            iptc_tags=iptc_tags,
            not_iptc_tags=not_iptc_tags,
            source_name=source_name,
            iab_tags=iab_tags,
            not_iab_tags=not_iab_tags,
            exclude_duplicates=exclude_duplicates,
            additional_domain_info=additional_domain_info,
            is_news_domain=is_news_domain,
            news_domain_type=news_domain_type,
            news_type=news_type,
        )
        if validate:
            return RootModel[SearchGetResponsePydantic](raw_response.body).root
        return api_client.construct_model_instance(SearchGetResponsePydantic, raw_response.body)


class ApiForget(BaseApi):
    # this class is used by api classes that refer to endpoints by path and http method names

    async def aget(
        self,
        q: str,
        search_in: typing.Optional[str] = None,
        predefined_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_author_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        from_: typing.Optional[typing.Union[str, datetime]] = None,
        to_: typing.Optional[typing.Union[str, datetime]] = None,
        published_date_precision: typing.Optional[str] = None,
        by_parse_date: typing.Optional[bool] = None,
        sort_by: typing.Optional[str] = None,
        ranked_only: typing.Optional[typing.Union[str, bool]] = None,
        from_rank: typing.Optional[int] = None,
        to_rank: typing.Optional[int] = None,
        is_headline: typing.Optional[bool] = None,
        is_opinion: typing.Optional[bool] = None,
        is_paid_content: typing.Optional[bool] = None,
        parent_url: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_domain_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        word_count_min: typing.Optional[int] = None,
        word_count_max: typing.Optional[int] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
        clustering_variable: typing.Optional[str] = None,
        clustering_enabled: typing.Optional[bool] = None,
        clustering_threshold: typing.Optional[typing.Union[int, float]] = None,
        include_nlp_data: typing.Optional[bool] = None,
        has_nlp: typing.Optional[bool] = None,
        theme: typing.Optional[str] = None,
        not_theme: typing.Optional[str] = None,
        org_entity_name: typing.Optional[str] = None,
        per_entity_name: typing.Optional[str] = None,
        loc_entity_name: typing.Optional[str] = None,
        misc_entity_name: typing.Optional[str] = None,
        title_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        title_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        source_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        exclude_duplicates: typing.Optional[bool] = None,
        additional_domain_info: typing.Optional[bool] = None,
        is_news_domain: typing.Optional[bool] = None,
        news_domain_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        news_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        **kwargs,
    ) -> typing.Union[
        ApiResponseFor200Async,
        api_client.ApiResponseWithoutDeserializationAsync,
        AsyncGeneratorResponse,
    ]:
        args = self._get_mapped_args(
            q=q,
            search_in=search_in,
            predefined_sources=predefined_sources,
            sources=sources,
            not_sources=not_sources,
            lang=lang,
            not_lang=not_lang,
            countries=countries,
            not_countries=not_countries,
            not_author_name=not_author_name,
            from_=from_,
            to_=to_,
            published_date_precision=published_date_precision,
            by_parse_date=by_parse_date,
            sort_by=sort_by,
            ranked_only=ranked_only,
            from_rank=from_rank,
            to_rank=to_rank,
            is_headline=is_headline,
            is_opinion=is_opinion,
            is_paid_content=is_paid_content,
            parent_url=parent_url,
            all_links=all_links,
            all_domain_links=all_domain_links,
            word_count_min=word_count_min,
            word_count_max=word_count_max,
            page=page,
            page_size=page_size,
            clustering_variable=clustering_variable,
            clustering_enabled=clustering_enabled,
            clustering_threshold=clustering_threshold,
            include_nlp_data=include_nlp_data,
            has_nlp=has_nlp,
            theme=theme,
            not_theme=not_theme,
            org_entity_name=org_entity_name,
            per_entity_name=per_entity_name,
            loc_entity_name=loc_entity_name,
            misc_entity_name=misc_entity_name,
            title_sentiment_min=title_sentiment_min,
            title_sentiment_max=title_sentiment_max,
            content_sentiment_min=content_sentiment_min,
            content_sentiment_max=content_sentiment_max,
            iptc_tags=iptc_tags,
            not_iptc_tags=not_iptc_tags,
            source_name=source_name,
            iab_tags=iab_tags,
            not_iab_tags=not_iab_tags,
            exclude_duplicates=exclude_duplicates,
            additional_domain_info=additional_domain_info,
            is_news_domain=is_news_domain,
            news_domain_type=news_domain_type,
            news_type=news_type,
        )
        return await self._aget_oapg(
            query_params=args.query,
            **kwargs,
        )
    
    def get(
        self,
        q: str,
        search_in: typing.Optional[str] = None,
        predefined_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_sources: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_lang: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_countries: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_author_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        from_: typing.Optional[typing.Union[str, datetime]] = None,
        to_: typing.Optional[typing.Union[str, datetime]] = None,
        published_date_precision: typing.Optional[str] = None,
        by_parse_date: typing.Optional[bool] = None,
        sort_by: typing.Optional[str] = None,
        ranked_only: typing.Optional[typing.Union[str, bool]] = None,
        from_rank: typing.Optional[int] = None,
        to_rank: typing.Optional[int] = None,
        is_headline: typing.Optional[bool] = None,
        is_opinion: typing.Optional[bool] = None,
        is_paid_content: typing.Optional[bool] = None,
        parent_url: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        all_domain_links: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        word_count_min: typing.Optional[int] = None,
        word_count_max: typing.Optional[int] = None,
        page: typing.Optional[int] = None,
        page_size: typing.Optional[int] = None,
        clustering_variable: typing.Optional[str] = None,
        clustering_enabled: typing.Optional[bool] = None,
        clustering_threshold: typing.Optional[typing.Union[int, float]] = None,
        include_nlp_data: typing.Optional[bool] = None,
        has_nlp: typing.Optional[bool] = None,
        theme: typing.Optional[str] = None,
        not_theme: typing.Optional[str] = None,
        org_entity_name: typing.Optional[str] = None,
        per_entity_name: typing.Optional[str] = None,
        loc_entity_name: typing.Optional[str] = None,
        misc_entity_name: typing.Optional[str] = None,
        title_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        title_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_min: typing.Optional[typing.Union[int, float]] = None,
        content_sentiment_max: typing.Optional[typing.Union[int, float]] = None,
        iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iptc_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        source_name: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        not_iab_tags: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        exclude_duplicates: typing.Optional[bool] = None,
        additional_domain_info: typing.Optional[bool] = None,
        is_news_domain: typing.Optional[bool] = None,
        news_domain_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
        news_type: typing.Optional[typing.Union[bool, date, datetime, dict, float, int, list, str, None]] = None,
    ) -> typing.Union[
        ApiResponseFor200,
        api_client.ApiResponseWithoutDeserialization,
    ]:
        """ This endpoint allows you to search for articles. You can search for articles by keyword, language, country, source, and more. """
        args = self._get_mapped_args(
            q=q,
            search_in=search_in,
            predefined_sources=predefined_sources,
            sources=sources,
            not_sources=not_sources,
            lang=lang,
            not_lang=not_lang,
            countries=countries,
            not_countries=not_countries,
            not_author_name=not_author_name,
            from_=from_,
            to_=to_,
            published_date_precision=published_date_precision,
            by_parse_date=by_parse_date,
            sort_by=sort_by,
            ranked_only=ranked_only,
            from_rank=from_rank,
            to_rank=to_rank,
            is_headline=is_headline,
            is_opinion=is_opinion,
            is_paid_content=is_paid_content,
            parent_url=parent_url,
            all_links=all_links,
            all_domain_links=all_domain_links,
            word_count_min=word_count_min,
            word_count_max=word_count_max,
            page=page,
            page_size=page_size,
            clustering_variable=clustering_variable,
            clustering_enabled=clustering_enabled,
            clustering_threshold=clustering_threshold,
            include_nlp_data=include_nlp_data,
            has_nlp=has_nlp,
            theme=theme,
            not_theme=not_theme,
            org_entity_name=org_entity_name,
            per_entity_name=per_entity_name,
            loc_entity_name=loc_entity_name,
            misc_entity_name=misc_entity_name,
            title_sentiment_min=title_sentiment_min,
            title_sentiment_max=title_sentiment_max,
            content_sentiment_min=content_sentiment_min,
            content_sentiment_max=content_sentiment_max,
            iptc_tags=iptc_tags,
            not_iptc_tags=not_iptc_tags,
            source_name=source_name,
            iab_tags=iab_tags,
            not_iab_tags=not_iab_tags,
            exclude_duplicates=exclude_duplicates,
            additional_domain_info=additional_domain_info,
            is_news_domain=is_news_domain,
            news_domain_type=news_domain_type,
            news_type=news_type,
        )
        return self._get_oapg(
            query_params=args.query,
        )

