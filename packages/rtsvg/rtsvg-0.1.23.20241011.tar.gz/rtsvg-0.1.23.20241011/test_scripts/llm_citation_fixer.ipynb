{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', 'Nemo-Instruct')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\n",
    "\n",
    "from mistral_inference.transformer import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\n",
    "model = Transformer.from_folder(mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promptModel(prompt, max_tokens=512, temperature=0.0):\n",
    "    completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n",
    "    tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "    out_tokens, _ = generate([tokens], model, max_tokens=max_tokens, temperature=temperature, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "    result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "    return result\n",
    "#prompt = \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\n",
    "#completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n",
    "#tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "#out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "#result = tokenizer.decode(out_tokens[0])\n",
    "#print(result)\n",
    "promptModel('How many \"r\"s are there in strawberry?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, original_citations, shortened_citations = [], [], []\n",
    "#\n",
    "# Source For Original Citations: https://en.wikipedia.org/wiki/Apollo_13\n",
    "#\n",
    "texts               .append('Apollo 13 was led by J. Lovell.')\n",
    "original_citations  .append('The mission was commanded by Jim Lovell, with Jack Swigert as command module (CM) pilot and Fred Haise as lunar module (LM) pilot. Swigert was a late replacement for Ken Mattingly, who was grounded after exposure to rubella. ')\n",
    "shortened_citations .append('The mission was commanded by Jim Lovell')\n",
    "\n",
    "texts               .append('Apollo looped around the Moon instead.')\n",
    "original_citations  .append('The crew, supported by backup systems on the lunar module (LM), instead looped around the Moon in a circumlunar trajectory and returned safely to Earth on April 17.')\n",
    "shortened_citations .append('instead looped around the Moon')\n",
    "\n",
    "texts               .append('They returned safely to Earth.')\n",
    "original_citations  .append('The crew, supported by backup systems on the lunar module (LM), instead looped around the Moon in a circumlunar trajectory and returned safely to Earth on April 17.')\n",
    "shortened_citations .append('returned safely to Earth')\n",
    "\n",
    "#\n",
    "# Source for Original Citations:  https://en.wikipedia.org/wiki/War_and_Peace\n",
    "#\n",
    "texts               .append('Authors did not always agree upon what comprised a novel -- for example, Tolstoy indicated that War and Peace was not a novel.')\n",
    "original_citations  .append('Tolstoy said that the best Russian literature does not conform to standards and hence hesitated to classify War and Peace, saying it is \"not a novel, even less is it a poem, and still less a historical chronicle.\"')\n",
    "shortened_citations .append('Tolstoy said that the best Russian literature does not conform to standards and hence hesitated to classify War and Peace, saying it is \"not a novel')\n",
    "\n",
    "#texts               .append('')\n",
    "#original_citations  .append('')\n",
    "#shortened_citations .append('')\n",
    "\n",
    "def cleanResponse(s):\n",
    "    if s.startswith(\"'\") and s.endswith(\"'\"): return s[1:-1]\n",
    "    if s.startswith('\"') and s.endswith('\"'): return s[1:-1]\n",
    "    return s\n",
    "\n",
    "def formatPrompt(text, original_citation):\n",
    "    return 'not defined yet'\n",
    "\n",
    "def runPrompts():\n",
    "    _responses_, max_response_lenght = [], 0\n",
    "    for i in range(len(texts)):\n",
    "        _text_              = texts[i]\n",
    "        _original_citation_ = original_citations[i]\n",
    "        _model_response_    = promptModel(formatPrompt(_text_, _original_citation_))\n",
    "        _model_response_    = cleanResponse(_model_response_)\n",
    "        _responses_.append(_model_response_)\n",
    "        max_response_lenght = max(max_response_lenght, len(_model_response_))\n",
    "    for i in range(len(texts)):\n",
    "        _model_response_    = _responses_[i]\n",
    "        _original_citation_ = original_citations[i]\n",
    "        _spaces_            = (max_response_lenght - len(_model_response_)) * ' '\n",
    "        print(f'\"{_model_response_}\" {_spaces_} {_model_response_ in _original_citation_}  \"{shortened_citations[i]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPrompt(text, original_citation): \n",
    "    return  \"My citations are too long.  \" + \\\n",
    "            \"Shorten the following citation to just the part that supports the phrase.  \" + \\\n",
    "           f\"The citation should use the exact words in the supplied citation.\\n\\nPhrase: '{text}'\\n\\nSupplied Citation: '{original_citation}'\"\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT Recommended Prompt \"make me a prompt for a language model that takes a citation that is too long and shortens it to only the necessary part for a given text.\"\n",
    "def formatPrompt(text, original_citation): return f'''Given a citation that is too lengthy, shorten it to include only the essential information needed to support the main point of the following text. Ensure the shortened citation retains the key details, remains accurate, and provides sufficient context.\n",
    "\n",
    "Text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Original Citation:\n",
    "\n",
    "{original_citation}\n",
    "\n",
    "Shortened Citation:'''\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude Recommendation (heavily modified to remove formal citatino language)\n",
    "def formatPrompt(text, original_citation): return f\"\"\"You are an AI assistant specialized in academic writing and citation management. Your task is to analyze a given piece of text and a lengthy citation, then shorten the citation to include only the parts that are directly relevant to the text.\n",
    "\n",
    "Text:\n",
    "'{text}'\n",
    "\n",
    "Lengthy Citation:\n",
    "'{original_citation}'\n",
    "\n",
    "Shortened Citation:\"\"\"\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
